{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses](http://www.menesesabad.com) for PyData 2018. Source and license info is on [GitHub](https://github.com/sorice/nlp_pydata2018/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de Textos con Python\n",
    "\n",
    "[Ver proceso previo: Corrección Ortográfica](#02.2-Spell-Checking.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indice'></a>\n",
    "##Índice##\n",
    "\n",
    "1. [Normalización de Textos](#text_normalization)\n",
    "    - 1.1 [Relacionado con los signos de puntuación](#punctuation_sign)\n",
    "    - 1.2 [Relacionado con los tokens especiales](#special_tokens)\n",
    "        - 1.2.1 [Correos Electrónicos y Frasis Multipalabras](#correos_electronicos)\n",
    "        - 1.2.2 [URLs](#urls)\n",
    "        - 1.2.3 [Siglas y Abreviaturas](#siglas_y_abreviaturas)\n",
    "    - 1.3 [Ralacionado con las palabras vacías de significado](#stop_words)\n",
    "    - 1.4 [Relacionado con cambios estructurales](#structural_normalization)\n",
    "\n",
    "2. [Flujo del Proceso de Normalización](#normalization_process_flow)\n",
    "3. [Análisis de Resultados](#results_analysis)\n",
    "\n",
    "[Ejercicios](#ejercicios)\n",
    "\n",
    "[Referencias](#references)\n",
    "\n",
    "[Índice Alfabético](#alphabetic_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text_normalization'></a>\n",
    "##Normalización de Textos##\n",
    "\n",
    "La **Normalización de Textos** *(text normalization)*: es el subproceso que implica mezclar\n",
    "diferentes formas de escritura en una sola apropiada y aceptable; por ejemplo un \n",
    "documento puede contener los símbolos “Señor”, “señor”, “Sr.”,”Sr” todos ellos \n",
    "deben ser normalizados a una única forma.[[1](#Indurkhya2008)]\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "* El signo más importante es el **punto final**. (Abel2015)\n",
    "* El segundo signo más importante es el **underscore** o \"_\". Este permite marcar \n",
    "[collocations](#collocations) para el posterior procesamiento del texto.\n",
    "* Un espacio en blanco antes y después de cada punto final descomplejiza las expresiones regulares.\n",
    "(Abel2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparando el ambiente para el pre-procesamiento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "LETTERS = ''.join([string.ascii_letters, string.digits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='punctuation_sign'></a>\n",
    "##Signos de Puntuación##\n",
    "\n",
    "<a id='urls'></a>\n",
    "###Signos fuera de rango ASCII y Latin1###\n",
    "\n",
    "Aquí otro dilema importante son las comillas simples y dobles, cuya significación aún es dudosa, pero\n",
    "que es necesario filtrar porque puede introducir caracteres extraños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punctuation_filter(text):\n",
    "   text = re.sub(\n",
    "                 u'(?:\\xc2|\\xa0)|'\n",
    "                 u'(?:\\\\xe2\\\\x80\\\\x9d|\\\\xe2\\\\x80\\\\x9c)|'       #Del “” en ascii.\n",
    "                 u'(?:\\u201c|\\u201d)|'                         #Del “” en utf8.\n",
    "                 u'(?:[\"]|[\\'])'                               #Del comillas dobles y simples sin decodificar.\n",
    "                 ,' ',text)\n",
    "   text = re.sub(u'(?:\\\\xe2\\\\x80\\\\x99|\\\\xe2\\\\x80\\\\x98)|'       # Del ‘’ en ascii.\n",
    "                 u'(?:\\u2018|\\u2019)'                          # Del ‘’ en ascii\n",
    "                 ,'\\'',text) \n",
    "   text = re.sub(u'(?:\\\\xe2\\\\x80\\\\x93)|'                       # Elimina guion largo ó – en ascii.\n",
    "                 u'(?:\\u2013)'                                 #Guión largo codificación utf8.\n",
    "                 ,' - ',text)\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    **Nota**: Esta función debe ser actualizada con todos los signos que estén fuera del rango ascci \n",
    "y Latin1 de lo contrario la sección del flujo dará un error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_puntos'></a>\n",
    "###Los 3 puntos seguidos ...###\n",
    "\n",
    "Para el análisis semántico algo importante son los **puntos finales** de oración. Sin embargo\n",
    "para el tratamiento con expresiones regulares los tres puntos es un signo muy complejo.\n",
    "Aunque aún no está claro cuál sería el patrón por el cual sustituirlo con el siguiente código\n",
    "se eliminan.\n",
    "\n",
    "**Nota**: este fue un código problemático, hice 3 implementaciones hasta que esta funcionó \n",
    "correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_contiguous_point_support(text):\n",
    "   for i in re.finditer('[.]\\s*?[.]+?[\\s|[.]]*',text):\n",
    "      for j in range(i.start(),i.end()):\n",
    "         if text[j] == '.' or text[j]==' ':\n",
    "            text = text[:j]+' '+text[j+1:]\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='special_tokens'></a>\n",
    "###Tokens Especiales###\n",
    "\n",
    "**Cambios a nivel morfológico y léxico.**\n",
    "\n",
    "<a id='correos_electronicos'></a>\n",
    "###Correos Electrónicos y Expresiones Multipalabras###\n",
    "\n",
    "Algunos tokens como los correos electrónicos **pedro@gmail.com**, o **enseñanza - aprendizaje**,\n",
    "**Firefox-v0.8** deben ser mantenidas por su valor semántico ya sea como sustantivos o sintagmas\n",
    "nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contiguos_string_recognition_support(text):\n",
    "   text = re.sub('\\n-','\\n- ',text)\n",
    "   # support for email address is inside the regexp\n",
    "   for i in re.finditer('[.]\\w*|-\\w*|@\\w*',text): \n",
    "      for j in range(i.start(),i.end()):\n",
    "         if j<(len(text)-1) and text[j] in string.punctuation and text[\n",
    "         j+1] not in string.whitespace:\n",
    "            text = text[:j]+'_'+text[j+1:]\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='urls'></a>\n",
    "###URLs###\n",
    "\n",
    "Otro token especial son las *URLs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_string_recognition_support(text):\n",
    "   for i in re.finditer('www\\S*(?=[.]+?\\s+?)|www\\S*(?=\\s+?)|http\\S*(?=[.]+?\\s+?)'\n",
    "                        +'|http\\S*(?=\\s+?)',text):\n",
    "      for j in range(i.start(),i.end()):\n",
    "         if text[j] in string.punctuation:\n",
    "            text = text[:j]+'_'+text[j+1:]\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función se analizan dos situaciones URL seguida por espacio (**Expr.** *www\\S*(?=\\s+?)*),\n",
    "y URL como token final(**Expr.** *www\\S*(?=[.]+?\\s+?)*) de una oración **Ej.**: **... www.google.com.*\n",
    "\n",
    "**Nota**: es importante que al final de la cadena parseada(*text*) haya al menos un espacio. \n",
    "Así en el caso de:\n",
    "*\"text = 'www.google.com'\"* las expresiones regulares tendrían que identificar que *'m'* es también\n",
    "el final de la cadena. \n",
    "Esto haría más compleja la función de reconocimiento; cuando en realidad se\n",
    "resuelve agregando un espacio al final de la cadena, antes de analizarla. \n",
    "Esto es muy sencillo de implementar en el flujo (ver como **Ej.** sección \n",
    "[add_text_end_dot](#add_text_end_dot))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='siglas_y_abreviaturas'></a>\n",
    "###Siglas y Abreviaturas###\n",
    "\n",
    "Un tipo de token especial son las **siglas, abreviaturas y otros similares**. En este aspecto ha de necesitarse\n",
    "un diccionario bien pulido, o tal vez un buen algoritmo para reconocer algunos. Sin embargo hay \n",
    "varios diccionarios, como el de libreoffice que pudieran utilizarse y mejorarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abbrev_recognition_support(text):\n",
    "   for i in re.finditer('Dr(?=[.]+?)|Ms.C(?=[.]+?)|Ph.D(?=[.]+?)|Ing(?=[.]+?)|Lic(?=[.]+?)',\n",
    "                        text):\n",
    "      text = text[:i.end()]+'_'+text[i.end()+1:]\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hipótesis**: Los algoritmos para buscar una cadena en una lista o diccionario pueden ser algo más lentos\n",
    "que las expresiones regulares. Esto es debido a que se necesita hacer un search sobre una estructura\n",
    "de datos una vez por cada token, en las expresiones regulares se revisa y sustituye en el texto\n",
    "completo una vez por cada patrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "#Pendiente versión 2 con diccionario de LibreOffice o de Google Translator.\n",
    "abbr = open('data/abbr').read()\n",
    "abbrDict = {}\n",
    "pattern = ':'\n",
    "for word in abbr.split('\\n'):\n",
    "    abbrDict[word] = word\n",
    "print (len(abbrDict))\n",
    "\n",
    "def abbr_filter(text, dic):\n",
    "    ntext = ''\n",
    "    for word in text.split(' '):\n",
    "        if word in dic:\n",
    "            word = dic[word]\n",
    "        ntext = ntext + word + '_'    \n",
    "    return ntext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Profiling de detección de siglas###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11484\n"
     ]
    }
   ],
   "source": [
    "from time import clock\n",
    "text = '' #Construyendo un texto de prueba.\n",
    "for word in abbrDict:\n",
    "    text += word+' '\n",
    "for n in range(2):\n",
    "    text += text\n",
    "\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expr\n",
      "10000 loops, best of 3: 105 µs per loop\n",
      "Tiempo basado en expresiones regulares 4.5335\n"
     ]
    }
   ],
   "source": [
    "print ('Expr')\n",
    "start_time1=clock()\n",
    "%timeit abbrev_recognition_support(text)\n",
    "end_time1=clock()-start_time1\n",
    "print ('Tiempo basado en expresiones regulares %.4f' %end_time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict\n",
      "1000 loops, best of 3: 1.07 ms per loop\n",
      "Tiempo basado en uso de diccionarios 4.5355\n"
     ]
    }
   ],
   "source": [
    "print ('Dict')\n",
    "start_time2=clock()\n",
    "%timeit abbr_filter(text,abbrDict)\n",
    "end_time2=clock()-start_time2\n",
    "print ('Tiempo basado en uso de diccionarios %.4f' %end_time2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Resultados'></a>\n",
    "###Resultados###\n",
    "\n",
    "Efectivamente la búsqueda de siglas basada en diccionarios es 10 veces más lenta que basada en\n",
    "expresiones regulares, evaluado en un contexto de más de 11000 términos, lo cual equivale al tamaño\n",
    "de un libro promedio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fechar'></a>\n",
    "###Formatos de Fechas###\n",
    "\n",
    "Otro token especial son las fechas **dd/mm/yy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pendiente de implementación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='stop_words'></a>\n",
    "###Palabras vacías###\n",
    "\n",
    "Aunque las palabras vacías son en esencia tokens sin significado dentro de la oración, y que actúan\n",
    "generalmente como conectores, los separamos por su importancia en el PLN. Fundamentalmente en el\n",
    "análisis de la eficiencia computacional y la eficiencia de los resultados de la similaridad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Los caracteres sueltos siempre son eliminables, al menos en español.\n",
    "def del_char_len_one(text):\n",
    "   text = re.sub('\\s\\w\\s',' ',text)\n",
    "   return text\n",
    "\n",
    "#Ver en el notebook de NLP una función más amplia utilizando NLTK de como\n",
    "# eliminar stop words o palabras vacías de más de un caracter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='structural_normalization'></a>\n",
    "##Normalización Estructural##\n",
    "\n",
    "El siguiente código solo marca el texto con un punto al final de la última oración, para evitar\n",
    "dificultades a la hora de reconocer todas las oraciones.\n",
    "\n",
    "<a id='add_text_end_dot'></a>\n",
    "###add_text_end_dot###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_text_end_dot(text):\n",
    "   end = len(text)-1\n",
    "   i = 0\n",
    "   while text[end] not in LETTERS:\n",
    "      end-=1\n",
    "      if text[end] == '.':\n",
    "         text = text[0:end]\n",
    "         i+=1\n",
    "   # si ningún caracter del final antes de letras o números es punto, ents suma un '.'\n",
    "   if i==0: \n",
    "      text += '.' \n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='normalization_process_flow'></a>\n",
    "##Flujo del Proceso de Normalización##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "processing urls\n",
      "processing some special punctuation signs\n",
      "clean contiguous dots\n",
      "abbrev recognition and normalization\n",
      "contiguous string recognition\n",
      "- Limpiando los signos de puntuación.\n",
      "-----LIMPIEZA-------------:  0.01045370101928711\n",
      "El tipo de datos de tokens es: <class 'list'>\n",
      "La cantidad de tokens después de limpiar es:  886 \n",
      "Eliminados 42 tokens durante la limpieza. \n",
      " Eliminados únicos:  -28\n",
      "La cantidad de términos únicos al filtrar es:  346\n",
      "Finalizado en  0.01274251937866211\n",
      "Fri Sep  2 14:47:59 2016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer\n",
    "from preprocess.punctuation import Replacer\n",
    "\n",
    "inita = time.time()\n",
    "doc_name = 'srctnlp1'\n",
    "text = open('test/2.3/'+doc_name+'.txt','r').read()\n",
    "print('---------')\n",
    "#Contar los términos únicos\n",
    "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "tokensa = tokenizer.tokenize(text)\n",
    "tokens_uniqueA = set(tokensa)\n",
    "\n",
    "#-------------------Special tokens recognition and normalization\n",
    "initg = time.time()\n",
    "text = open('test/2.3/'+doc_name+'.txt','r').read()\n",
    "print ('processing urls')\n",
    "text = url_string_recognition_support(text)\n",
    "print ('processing some special punctuation signs')\n",
    "text = punctuation_filter(text)\n",
    "print ('clean contiguous dots')\n",
    "text = del_contiguous_point_support(text)\n",
    "print ('abbrev recognition and normalization')\n",
    "#~ text = abbrev_recognition_support(text)\n",
    "print ('contiguous string recognition')\n",
    "# Esta demora mucho, hay que ver porque\n",
    "text = contiguos_string_recognition_support(text) \n",
    "\n",
    "texto = open('test/2.3/out_'+doc_name+'1_normalized_tokens.txt', 'w')\n",
    "texto.write(text)\n",
    "texto.close()\n",
    "\n",
    "#-------------------Clean all punctuation sign\n",
    "print ('- Limpiando los signos de puntuación.')\n",
    "text = open('test/2.3/out_'+doc_name+'1_normalized_tokens.txt','r').read()\n",
    "replacer = Replacer()\n",
    "chunk = replacer.replace(text)\n",
    "\n",
    "texto = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt','w')\n",
    "texto.write(chunk)\n",
    "texto.close()\n",
    "\n",
    "text = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt','r').read()\n",
    "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "#Contando los términos únicos\n",
    "tokens_uniqueD = set(tokens)\n",
    "\n",
    "timeg = time.time() - initg\n",
    "\n",
    "print ('-----LIMPIEZA-------------: ', timeg)\n",
    "print ('El tipo de datos de tokens es:', type(tokens))\n",
    "print (\"La cantidad de tokens después de limpiar es: \", len(tokens),\n",
    "\"\\nEliminados \"+str(len(tokens)-len(tokensa))+\" tokens durante la limpieza.\",\n",
    "\"\\n Eliminados únicos: \", len(tokens_uniqueD)-len(tokens_uniqueA))\n",
    "\n",
    "text = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt', 'r').read()\n",
    "text = add_text_end_dot(text)\n",
    "\n",
    "texto = open('test/2.3/out_'+doc_name+'6_clean_punctuation.txt', 'w')\n",
    "texto.write(text)\n",
    "texto.close()\n",
    "\n",
    "timefa = time.time() - inita\n",
    "print ('La cantidad de términos únicos al filtrar es: ', len(tokens_uniqueD))\n",
    "\n",
    "print ('Finalizado en ', timefa)\n",
    "print (time.ctime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='results_analysis'></a>\n",
    "##Análisis de Resultados##\n",
    "\n",
    "Veamos que tal el resultado, versus el resultado hecho por un ser humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACID \n",
      "ACID \n",
      "-----\n",
      " En bases de datos se denomina ACID a un conjunto de características necesarias para que una serie de instrucciones puedan ser consideradas como una transacción \n",
      " \n",
      "En bases de datos se denomina ACID a un conjunto de características necesarias para que una serie de instrucciones puedan ser consideradas como una transacción \n",
      "-----\n",
      " Así pues si un sistema de gestión de bases de datos es ACID compliant quiere decir que el mismo cuenta con las funcionalidades necesarias para que sus transacciones tengan las características ACID \n",
      " Así pues, si un sistema de gestión de bases de datos es ACID compliant quiere decir que el mismo cuenta con las funcionalidades necesarias para que sus transacciones tengan las características ACID\n",
      "-----\n",
      " En concreto ACID es un acrónimo de Atomicity Consistency Isolation and Durability \n",
      "\n",
      "\n",
      "En concreto ACID es un acrónimo de Atomicity, Consistency, Isolation and Durability\n",
      "-----\n",
      " Atomicidad Consistencia Aislamiento y Durabilidad en español \n",
      " Atomicidad, Consistencia, Aislamiento y Durabilidad en español\n",
      "-----\n",
      " Definiciones \n",
      "\n",
      "\n",
      "Definiciones \n",
      "-----\n",
      " Atomicidad es la propiedad que asegura que la operación se ha realizado o no y por lo tanto ante un fallo del sistema no puede quedar a medias \n",
      " \n",
      "- Atomicidad: es la propiedad que asegura que la operación se ha realizado o no, y por lo tanto ante un fallo del sistema no puede quedar a medias\n",
      "-----\n",
      " Se dice que una operación es atómica cuando es imposible para otra parte de un sistema encontrar pasos intermedios \n",
      " Se dice que una operación es atómica cuando es imposible para otra parte de un sistema encontrar pasos intermedios\n",
      "-----\n",
      " Si esta operación consiste en una serie de pasos todos ellos ocurren o ninguno \n",
      " Si esta operación consiste en una serie de pasos, todos ellos ocurren o ninguno\n",
      "-----\n",
      " Por ejemplo en el caso de una transacción bancaria o se ejecuta tanto el depósito como la deducción o ninguna acción es realizada \n",
      " Por ejemplo, en el caso de una transacción bancaria o se ejecuta tanto el depósito como la deducción o ninguna acción es realizada\n",
      "-----\n",
      " Consistencia \n",
      "\n",
      "- Consistencia\n",
      "-----\n",
      " Integridad \n",
      " Integridad\n",
      "-----\n",
      " Es la propiedad que asegura que sólo se empieza aquello que se puede acabar \n",
      " Es la propiedad que asegura que sólo se empieza aquello que se puede acabar\n",
      "-----\n",
      " Por lo tanto se ejecutan aquellas operaciones que no van a romper las reglas y directrices de integridad de la base de datos \n",
      " Por lo tanto se ejecutan aquellas operaciones que no van a romper las reglas y directrices de integridad de la base de datos\n",
      "-----\n",
      " La propiedad de consistencia sostiene que cualquier transacción llevará a la base de datos desde un estado válido a otro también válido \n",
      " La propiedad de consistencia sostiene que cualquier transacción llevará a la base de datos desde un estado válido a otro también válido\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "textout = open('test/2.3/out_'+doc_name+'6_clean_punctuation.txt').read()\n",
    "texthuman = open('test/2.3/'+doc_name+'_human_analysis.txt').read()\n",
    "lineout = []\n",
    "linehuman=[]\n",
    "\n",
    "for line in textout.split('.'):\n",
    "   lineout.append(line)\n",
    "for line in texthuman.split('.'):\n",
    "   linehuman.append(line)\n",
    "    \n",
    "for i in range(15):#max(len(lineout),len(linehuman))):\n",
    "   if i < len(lineout):\n",
    "        print (lineout[i])\n",
    "   if i < len(linehuman):\n",
    "        print (linehuman[i])\n",
    "   print  ('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='ejercicios'></a>\n",
    "## Ejercicios\n",
    "\n",
    "* **Ejercicio 1**: Sobre el filtrado y normalización de siglas, \n",
    "implemente una solución basada en heurísticas más eficiente que las mostradas en este material.\n",
    "* **Ejercicio 2**: Sobre el filtrado de tokens especiales, implemente una solución\n",
    "que reconozca formatos de fecha.\n",
    "* **Ejercicio 3**: Encuentre otros signos de puntuación fuera del rango ascii y Latin1, e implemente \n",
    "las expresiones regulares que eviten problemas en la codificación iso8859-1 para español.\n",
    "* **Ejercicio 4**: Implemente una solución con NLTK para el filtrado de stopwords.\n",
    "* **Ejercicio 5**: Cambie el orden de algunas de las codificaciones en el flujo de pre-procesamiento\n",
    "y vea que sucede para textos en idioma español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='references'></a>\n",
    "## Referencias\n",
    "\n",
    "<a id='Indurkhya2008'></a>\n",
    "[1] *[Indurkhya2008]* Nitin Indurkhya. Book **Handbook of Natural Language Processing**. 2008. \n",
    "p. 10 **ISBN**: 978-1-4200-8593-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='alphabetic_index'></a>\n",
    "## Índice Alfabético\n",
    "\n",
    "<a id='collocations'></a>\n",
    "**Collocations**: secuencia de palabras que aparecen juntas de forma frecuente, \n",
    "estableciéndose como nuevos códigos de la lengua. Ej. “caballero negro”, “vino blanco”, \n",
    "“Estados Unidos de Norteamérica”, etc.\n",
    "\n",
    "\n",
    "Volver al [*Índice*](#indice)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
