{
 "metadata": {
  "name": "",
  "signature": "sha256:4a3f4b8bb44df5d14963d27b4285f05cced4b866b2576a6a24fa066ad47a3d6b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Abel Meneses-Abad](http://www.meneses-abad.com) for PyData Vancouver 2018. Source and license info is on [GitHub](https://github.com/sorice/nlp_pydata2018/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Normalizaci\u00f3n de Textos con Python#\n",
      "\n",
      "[Ver proceso previo: Correcci\u00f3n Ortogr\u00e1fica](#02.2-Spell-Checking.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='indice'></a>\n",
      "##\u00cdndice##\n",
      "\n",
      "1. [Normalizaci\u00f3n de Textos](#text_normalization)\n",
      "    - 1.1 [Relacionado con los signos de puntuaci\u00f3n](#punctuation_sign)\n",
      "    - 1.2 [Relacionado con los tokens especiales](#special_tokens)\n",
      "        - 1.2.1 [Correos Electr\u00f3nicos y Frasis Multipalabras](#correos_electronicos)\n",
      "        - 1.2.2 [URLs](#urls)\n",
      "        - 1.2.3 [Siglas y Abreviaturas](#siglas_y_abreviaturas)\n",
      "    - 1.3 [Ralacionado con las palabras vac\u00edas de significado](#stop_words)\n",
      "    - 1.4 [Relacionado con cambios estructurales](#structural_normalization)\n",
      "\n",
      "2. [Flujo del Proceso de Normalizaci\u00f3n](#normalization_process_flow)\n",
      "3. [An\u00e1lisis de Resultados](#results_analysis)\n",
      "\n",
      "[Ejercicios](#ejercicios)\n",
      "\n",
      "[Referencias](#references)\n",
      "\n",
      "[\u00cdndice Alfab\u00e9tico](#alphabetic_index)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='text_normalization'></a>\n",
      "##Normalizaci\u00f3n de Textos##\n",
      "\n",
      "La **Normalizaci\u00f3n de Textos** *(text normalization)*: es el subproceso que implica mezclar\n",
      "diferentes formas de escritura en una sola apropiada y aceptable; por ejemplo un \n",
      "documento puede contener los s\u00edmbolos \u201cSe\u00f1or\u201d, \u201cse\u00f1or\u201d, \u201cSr.\u201d,\u201dSr\u201d todos ellos \n",
      "deben ser normalizados a una \u00fanica forma.[[1](#Indurkhya2008)]\n",
      "\n",
      "**Tips**:\n",
      "\n",
      "* El signo m\u00e1s importante es el **punto final**. (Abel2015)\n",
      "* El segundo signo m\u00e1s importante es el **underscore** o \"_\". Este permite marcar \n",
      "[collocations](#collocations) para el posterior procesamiento del texto.\n",
      "* Un espacio en blanco antes y despu\u00e9s de cada punto final descomplejiza las expresiones regulares.\n",
      "(Abel2015)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Preparando el ambiente para el pre-procesamiento.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import re\n",
      "import string\n",
      "\n",
      "LETTERS = ''.join([string.ascii_letters, string.digits])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='punctuation_sign'></a>\n",
      "##Signos de Puntuaci\u00f3n##\n",
      "\n",
      "<a id='urls'></a>\n",
      "###Signos fuera de rango ASCII y Latin1###\n",
      "\n",
      "Aqu\u00ed otro dilema importante son las comillas simples y dobles, cuya significaci\u00f3n a\u00fan es dudosa, pero\n",
      "que es necesario filtrar porque puede introducir caracteres extra\u00f1os."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def punctuation_filter(text):\n",
      "   text = re.sub(\n",
      "                 u'(?:\\xc2|\\xa0)|'\n",
      "                 u'(?:\\\\xe2\\\\x80\\\\x9d|\\\\xe2\\\\x80\\\\x9c)|'       #Del \u201c\u201d en ascii.\n",
      "                 u'(?:\\u201c|\\u201d)|'                         #Del \u201c\u201d en utf8.\n",
      "                 u'(?:[\"]|[\\'])'                               #Del comillas dobles y simples sin decodificar.\n",
      "                 ,' ',text)\n",
      "   text = re.sub(u'(?:\\\\xe2\\\\x80\\\\x99|\\\\xe2\\\\x80\\\\x98)|'       # Del \u2018\u2019 en ascii.\n",
      "                 u'(?:\\u2018|\\u2019)'                          # Del \u2018\u2019 en ascii\n",
      "                 ,'\\'',text) \n",
      "   text = re.sub(u'(?:\\\\xe2\\\\x80\\\\x93)|'                       # Elimina guion largo \u00f3 \u2013 en ascii.\n",
      "                 u'(?:\\u2013)'                                 #Gui\u00f3n largo codificaci\u00f3n utf8.\n",
      "                 ,' - ',text)\n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    **Nota**: Esta funci\u00f3n debe ser actualizada con todos los signos que est\u00e9n fuera del rango ascci \n",
      "y Latin1 de lo contrario la secci\u00f3n del flujo dar\u00e1 un error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='3_puntos'></a>\n",
      "###Los 3 puntos seguidos ...###\n",
      "\n",
      "Para el an\u00e1lisis sem\u00e1ntico algo importante son los **puntos finales** de oraci\u00f3n. Sin embargo\n",
      "para el tratamiento con expresiones regulares los tres puntos es un signo muy complejo.\n",
      "Aunque a\u00fan no est\u00e1 claro cu\u00e1l ser\u00eda el patr\u00f3n por el cual sustituirlo con el siguiente c\u00f3digo\n",
      "se eliminan.\n",
      "\n",
      "**Nota**: este fue un c\u00f3digo problem\u00e1tico, hice 3 implementaciones hasta que esta funcion\u00f3 \n",
      "correctamente."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def del_contiguous_point_support(text):\n",
      "   for i in re.finditer('[.]\\s*?[.]+?[\\s|[.]]*',text):\n",
      "      for j in range(i.start(),i.end()):\n",
      "         if text[j] == '.' or text[j]==' ':\n",
      "            text = text[:j]+' '+text[j+1:]\n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='special_tokens'></a>\n",
      "###Tokens Especiales###\n",
      "\n",
      "**Cambios a nivel morfol\u00f3gico y l\u00e9xico.**\n",
      "\n",
      "<a id='correos_electronicos'></a>\n",
      "###Correos Electr\u00f3nicos y Expresiones Multipalabras###\n",
      "\n",
      "Algunos tokens como los correos electr\u00f3nicos **pedro@gmail.com**, o **ense\u00f1anza - aprendizaje**,\n",
      "**Firefox-v0.8** deben ser mantenidas por su valor sem\u00e1ntico ya sea como sustantivos o sintagmas\n",
      "nominales."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def contiguos_string_recognition_support(text):\n",
      "   text = re.sub('\\n-','\\n- ',text)\n",
      "   # support for email address is inside the regexp\n",
      "   for i in re.finditer('[.]\\w*|-\\w*|@\\w*',text): \n",
      "      for j in range(i.start(),i.end()):\n",
      "         if j<(len(text)-1) and text[j] in string.punctuation and text[\n",
      "         j+1] not in string.whitespace:\n",
      "            text = text[:j]+'_'+text[j+1:]\n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='urls'></a>\n",
      "###URLs###\n",
      "\n",
      "Otro token especial son las *URLs*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def url_string_recognition_support(text):\n",
      "   for i in re.finditer('www\\S*(?=[.]+?\\s+?)|www\\S*(?=\\s+?)|http\\S*(?=[.]+?\\s+?)'\n",
      "                        +'|http\\S*(?=\\s+?)',text):\n",
      "      for j in range(i.start(),i.end()):\n",
      "         if text[j] in string.punctuation:\n",
      "            text = text[:j]+'_'+text[j+1:]\n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En esta funci\u00f3n se analizan dos situaciones URL seguida por espacio (**Expr.** *www\\S*(?=\\s+?)*),\n",
      "y URL como token final(**Expr.** *www\\S*(?=[.]+?\\s+?)*) de una oraci\u00f3n **Ej.**: **... www.google.com.*\n",
      "\n",
      "**Nota**: es importante que al final de la cadena parseada(*text*) haya al menos un espacio. \n",
      "As\u00ed en el caso de:\n",
      "*\"text = 'www.google.com'\"* las expresiones regulares tendr\u00edan que identificar que *'m'* es tambi\u00e9n\n",
      "el final de la cadena. \n",
      "Esto har\u00eda m\u00e1s compleja la funci\u00f3n de reconocimiento; cuando en realidad se\n",
      "resuelve agregando un espacio al final de la cadena, antes de analizarla. \n",
      "Esto es muy sencillo de implementar en el flujo (ver como **Ej.** secci\u00f3n \n",
      "[add_text_end_dot](#add_text_end_dot))."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='siglas_y_abreviaturas'></a>\n",
      "###Siglas y Abreviaturas###\n",
      "\n",
      "Un tipo de token especial son las **siglas, abreviaturas y otros similares**. En este aspecto ha de necesitarse\n",
      "un diccionario bien pulido, o tal vez un buen algoritmo para reconocer algunos. Sin embargo hay \n",
      "varios diccionarios, como el de libreoffice que pudieran utilizarse y mejorarse."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def abbrev_recognition_support(text):\n",
      "   for i in re.finditer('Dr(?=[.]+?)|Ms.C(?=[.]+?)|Ph.D(?=[.]+?)|Ing(?=[.]+?)|Lic(?=[.]+?)',\n",
      "                        text):\n",
      "      text = text[:i.end()]+'_'+text[i.end()+1:]\n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Hip\u00f3tesis**: Los algoritmos para buscar una cadena en una lista o diccionario pueden ser algo m\u00e1s lentos\n",
      "que las expresiones regulares. Esto es debido a que se necesita hacer un search sobre una estructura\n",
      "de datos una vez por cada token, en las expresiones regulares se revisa y sustituye en el texto\n",
      "completo una vez por cada patr\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Pendiente versi\u00f3n 2 con diccionario de LibreOffice o de Google Translator.\n",
      "abbr = open('data/abbr').read()\n",
      "abbrDict = {}\n",
      "pattern = ':'\n",
      "for word in abbr.split('\\n'):\n",
      "    abbrDict[word] = word\n",
      "print (len(abbrDict))\n",
      "\n",
      "def abbr_filter(text, dic):\n",
      "    ntext = ''\n",
      "    for word in text.split(' '):\n",
      "        if word in dic:\n",
      "            word = dic[word]\n",
      "        ntext = ntext + word + '_'    \n",
      "    return ntext"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "481\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Profiling de detecci\u00f3n de siglas###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import clock\n",
      "text = '' #Construyendo un texto de prueba.\n",
      "for word in abbrDict:\n",
      "    text += word+' '\n",
      "for n in range(2):\n",
      "    text += text\n",
      "\n",
      "print (len(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11484\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ('Expr')\n",
      "start_time1=clock()\n",
      "%timeit abbrev_recognition_support(text)\n",
      "end_time1=clock()-start_time1\n",
      "print ('Tiempo basado en expresiones regulares %.4f' %end_time1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Expr\n",
        "10000 loops, best of 3: 105 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Tiempo basado en expresiones regulares 4.5335\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ('Dict')\n",
      "start_time2=clock()\n",
      "%timeit abbr_filter(text,abbrDict)\n",
      "end_time2=clock()-start_time2\n",
      "print ('Tiempo basado en uso de diccionarios %.4f' %end_time2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dict\n",
        "1000 loops, best of 3: 1.07 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Tiempo basado en uso de diccionarios 4.5355\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='Resultados'></a>\n",
      "###Resultados###\n",
      "\n",
      "Efectivamente la b\u00fasqueda de siglas basada en diccionarios es 10 veces m\u00e1s lenta que basada en\n",
      "expresiones regulares, evaluado en un contexto de m\u00e1s de 11000 t\u00e9rminos, lo cual equivale al tama\u00f1o\n",
      "de un libro promedio."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='fechar'></a>\n",
      "###Formatos de Fechas###\n",
      "\n",
      "Otro token especial son las fechas **dd/mm/yy**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Pendiente de implementaci\u00f3n."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='stop_words'></a>\n",
      "###Palabras vac\u00edas###\n",
      "\n",
      "Aunque las palabras vac\u00edas son en esencia tokens sin significado dentro de la oraci\u00f3n, y que act\u00faan\n",
      "generalmente como conectores, los separamos por su importancia en el PLN. Fundamentalmente en el\n",
      "an\u00e1lisis de la eficiencia computacional y la eficiencia de los resultados de la similaridad."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Los caracteres sueltos siempre son eliminables, al menos en espa\u00f1ol.\n",
      "def del_char_len_one(text):\n",
      "   text = re.sub('\\s\\w\\s',' ',text)\n",
      "   return text\n",
      "\n",
      "#Ver en el notebook de NLP una funci\u00f3n m\u00e1s amplia utilizando NLTK de como\n",
      "# eliminar stop words o palabras vac\u00edas de m\u00e1s de un caracter."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='structural_normalization'></a>\n",
      "##Normalizaci\u00f3n Estructural##\n",
      "\n",
      "El siguiente c\u00f3digo solo marca el texto con un punto al final de la \u00faltima oraci\u00f3n, para evitar\n",
      "dificultades a la hora de reconocer todas las oraciones.\n",
      "\n",
      "<a id='add_text_end_dot'></a>\n",
      "###add_text_end_dot###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def add_text_end_dot(text):\n",
      "   end = len(text)-1\n",
      "   i = 0\n",
      "   while text[end] not in LETTERS:\n",
      "      end-=1\n",
      "      if text[end] == '.':\n",
      "         text = text[0:end]\n",
      "         i+=1\n",
      "   # si ning\u00fan caracter del final antes de letras o n\u00fameros es punto, ents suma un '.'\n",
      "   if i==0: \n",
      "      text += '.' \n",
      "   return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='normalization_process_flow'></a>\n",
      "##Flujo del Proceso de Normalizaci\u00f3n##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer\n",
      "from preprocess.punctuation import Replacer\n",
      "\n",
      "inita = time.time()\n",
      "doc_name = 'srctnlp1'\n",
      "text = open('test/2.3/'+doc_name+'.txt','r').read()\n",
      "print('---------')\n",
      "#Contar los t\u00e9rminos \u00fanicos\n",
      "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
      "tokensa = tokenizer.tokenize(text)\n",
      "tokens_uniqueA = set(tokensa)\n",
      "\n",
      "#-------------------Special tokens recognition and normalization\n",
      "initg = time.time()\n",
      "text = open('test/2.3/'+doc_name+'.txt','r').read()\n",
      "print ('processing urls')\n",
      "text = url_string_recognition_support(text)\n",
      "print ('processing some special punctuation signs')\n",
      "text = punctuation_filter(text)\n",
      "print ('clean contiguous dots')\n",
      "text = del_contiguous_point_support(text)\n",
      "print ('abbrev recognition and normalization')\n",
      "#~ text = abbrev_recognition_support(text)\n",
      "print ('contiguous string recognition')\n",
      "# Esta demora mucho, hay que ver porque\n",
      "text = contiguos_string_recognition_support(text) \n",
      "\n",
      "texto = open('test/2.3/out_'+doc_name+'1_normalized_tokens.txt', 'w')\n",
      "texto.write(text)\n",
      "texto.close()\n",
      "\n",
      "#-------------------Clean all punctuation sign\n",
      "print ('- Limpiando los signos de puntuaci\u00f3n.')\n",
      "text = open('test/2.3/out_'+doc_name+'1_normalized_tokens.txt','r').read()\n",
      "replacer = Replacer()\n",
      "chunk = replacer.replace(text)\n",
      "\n",
      "texto = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt','w')\n",
      "texto.write(chunk)\n",
      "texto.close()\n",
      "\n",
      "text = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt','r').read()\n",
      "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
      "tokens = tokenizer.tokenize(text)\n",
      "\n",
      "#Contando los t\u00e9rminos \u00fanicos\n",
      "tokens_uniqueD = set(tokens)\n",
      "\n",
      "timeg = time.time() - initg\n",
      "\n",
      "print ('-----LIMPIEZA-------------: ', timeg)\n",
      "print ('El tipo de datos de tokens es:', type(tokens))\n",
      "print (\"La cantidad de tokens despu\u00e9s de limpiar es: \", len(tokens),\n",
      "\"\\nEliminados \"+str(len(tokens)-len(tokensa))+\" tokens durante la limpieza.\",\n",
      "\"\\n Eliminados \u00fanicos: \", len(tokens_uniqueD)-len(tokens_uniqueA))\n",
      "\n",
      "text = open('test/2.3/out_'+doc_name+'2_tokens_including_points.txt', 'r').read()\n",
      "text = add_text_end_dot(text)\n",
      "\n",
      "texto = open('test/2.3/out_'+doc_name+'6_clean_punctuation.txt', 'w')\n",
      "texto.write(text)\n",
      "texto.close()\n",
      "\n",
      "timefa = time.time() - inita\n",
      "print ('La cantidad de t\u00e9rminos \u00fanicos al filtrar es: ', len(tokens_uniqueD))\n",
      "\n",
      "print ('Finalizado en ', timefa)\n",
      "print (time.ctime())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------\n",
        "processing urls\n",
        "processing some special punctuation signs\n",
        "clean contiguous dots\n",
        "abbrev recognition and normalization\n",
        "contiguous string recognition\n",
        "- Limpiando los signos de puntuaci\u00f3n.\n",
        "-----LIMPIEZA-------------:  0.01045370101928711\n",
        "El tipo de datos de tokens es: <class 'list'>\n",
        "La cantidad de tokens despu\u00e9s de limpiar es:  886 \n",
        "Eliminados 42 tokens durante la limpieza. \n",
        " Eliminados \u00fanicos:  -28\n",
        "La cantidad de t\u00e9rminos \u00fanicos al filtrar es:  346\n",
        "Finalizado en  0.01274251937866211\n",
        "Fri Sep  2 14:47:59 2016\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='results_analysis'></a>\n",
      "##An\u00e1lisis de Resultados##\n",
      "\n",
      "Veamos que tal el resultado, versus el resultado hecho por un ser humano."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "textout = open('test/2.3/out_'+doc_name+'6_clean_punctuation.txt').read()\n",
      "texthuman = open('test/2.3/'+doc_name+'_human_analysis.txt').read()\n",
      "lineout = []\n",
      "linehuman=[]\n",
      "\n",
      "for line in textout.split('.'):\n",
      "   lineout.append(line)\n",
      "for line in texthuman.split('.'):\n",
      "   linehuman.append(line)\n",
      "    \n",
      "for i in range(15):#max(len(lineout),len(linehuman))):\n",
      "   if i < len(lineout):\n",
      "        print (lineout[i])\n",
      "   if i < len(linehuman):\n",
      "        print (linehuman[i])\n",
      "   print  ('-----')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ACID \n",
        "ACID \n",
        "-----\n",
        " En bases de datos se denomina ACID a un conjunto de caracter\u00edsticas necesarias para que una serie de instrucciones puedan ser consideradas como una transacci\u00f3n \n",
        " \n",
        "En bases de datos se denomina ACID a un conjunto de caracter\u00edsticas necesarias para que una serie de instrucciones puedan ser consideradas como una transacci\u00f3n \n",
        "-----\n",
        " As\u00ed pues si un sistema de gesti\u00f3n de bases de datos es ACID compliant quiere decir que el mismo cuenta con las funcionalidades necesarias para que sus transacciones tengan las caracter\u00edsticas ACID \n",
        " As\u00ed pues, si un sistema de gesti\u00f3n de bases de datos es ACID compliant quiere decir que el mismo cuenta con las funcionalidades necesarias para que sus transacciones tengan las caracter\u00edsticas ACID\n",
        "-----\n",
        " En concreto ACID es un acr\u00f3nimo de Atomicity Consistency Isolation and Durability \n",
        "\n",
        "\n",
        "En concreto ACID es un acr\u00f3nimo de Atomicity, Consistency, Isolation and Durability\n",
        "-----\n",
        " Atomicidad Consistencia Aislamiento y Durabilidad en espa\u00f1ol \n",
        " Atomicidad, Consistencia, Aislamiento y Durabilidad en espa\u00f1ol\n",
        "-----\n",
        " Definiciones \n",
        "\n",
        "\n",
        "Definiciones \n",
        "-----\n",
        " Atomicidad es la propiedad que asegura que la operaci\u00f3n se ha realizado o no y por lo tanto ante un fallo del sistema no puede quedar a medias \n",
        " \n",
        "- Atomicidad: es la propiedad que asegura que la operaci\u00f3n se ha realizado o no, y por lo tanto ante un fallo del sistema no puede quedar a medias\n",
        "-----\n",
        " Se dice que una operaci\u00f3n es at\u00f3mica cuando es imposible para otra parte de un sistema encontrar pasos intermedios \n",
        " Se dice que una operaci\u00f3n es at\u00f3mica cuando es imposible para otra parte de un sistema encontrar pasos intermedios\n",
        "-----\n",
        " Si esta operaci\u00f3n consiste en una serie de pasos todos ellos ocurren o ninguno \n",
        " Si esta operaci\u00f3n consiste en una serie de pasos, todos ellos ocurren o ninguno\n",
        "-----\n",
        " Por ejemplo en el caso de una transacci\u00f3n bancaria o se ejecuta tanto el dep\u00f3sito como la deducci\u00f3n o ninguna acci\u00f3n es realizada \n",
        " Por ejemplo, en el caso de una transacci\u00f3n bancaria o se ejecuta tanto el dep\u00f3sito como la deducci\u00f3n o ninguna acci\u00f3n es realizada\n",
        "-----\n",
        " Consistencia \n",
        "\n",
        "- Consistencia\n",
        "-----\n",
        " Integridad \n",
        " Integridad\n",
        "-----\n",
        " Es la propiedad que asegura que s\u00f3lo se empieza aquello que se puede acabar \n",
        " Es la propiedad que asegura que s\u00f3lo se empieza aquello que se puede acabar\n",
        "-----\n",
        " Por lo tanto se ejecutan aquellas operaciones que no van a romper las reglas y directrices de integridad de la base de datos \n",
        " Por lo tanto se ejecutan aquellas operaciones que no van a romper las reglas y directrices de integridad de la base de datos\n",
        "-----\n",
        " La propiedad de consistencia sostiene que cualquier transacci\u00f3n llevar\u00e1 a la base de datos desde un estado v\u00e1lido a otro tambi\u00e9n v\u00e1lido \n",
        " La propiedad de consistencia sostiene que cualquier transacci\u00f3n llevar\u00e1 a la base de datos desde un estado v\u00e1lido a otro tambi\u00e9n v\u00e1lido\n",
        "-----\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='ejercicios'></a>\n",
      "##Ejercicios##\n",
      "\n",
      "* **Ejercicio 1**: Sobre el filtrado y normalizaci\u00f3n de siglas, \n",
      "implemente una soluci\u00f3n basada en heur\u00edsticas m\u00e1s eficiente que las mostradas en este material.\n",
      "* **Ejercicio 2**: Sobre el filtrado de tokens especiales, implemente una soluci\u00f3n\n",
      "que reconozca formatos de fecha.\n",
      "* **Ejercicio 3**: Encuentre otros signos de puntuaci\u00f3n fuera del rango ascii y Latin1, e implemente \n",
      "las expresiones regulares que eviten problemas en la codificaci\u00f3n iso8859-1 para espa\u00f1ol.\n",
      "* **Ejercicio 4**: Implemente una soluci\u00f3n con NLTK para el filtrado de stopwords.\n",
      "* **Ejercicio 5**: Cambie el orden de algunas de las codificaciones en el flujo de pre-procesamiento\n",
      "y vea que sucede para textos en idioma espa\u00f1ol."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='references'></a>\n",
      "##Referencias##\n",
      "\n",
      "<a id='Indurkhya2008'></a>\n",
      "[1] *[Indurkhya2008]* Nitin Indurkhya. Book **Handbook of Natural Language Processing**. 2008. \n",
      "p. 10 **ISBN**: 978-1-4200-8593-8"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='alphabetic_index'></a>\n",
      "##\u00cdndice Alfab\u00e9tico##\n",
      "\n",
      "<a id='collocations'></a>\n",
      "**Collocations**: secuencia de palabras que aparecen juntas de forma frecuente, \n",
      "estableci\u00e9ndose como nuevos c\u00f3digos de la lengua. Ej. \u201ccaballero negro\u201d, \u201cvino blanco\u201d, \n",
      "\u201cEstados Unidos de Norteam\u00e9rica\u201d, etc.\n",
      "\n",
      "\n",
      "Volver al [*\u00cdndice*](#indice)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}