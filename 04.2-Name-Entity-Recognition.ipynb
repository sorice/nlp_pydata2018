{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses](http://www.menesesabad.com) for PyData 2018. Source and license info is on [GitHub](https://github.com/sorice/nlp_pydata2018/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nerc'></a>\n",
    "# Named Entity Recognition (NER)\n",
    "\n",
    "## NER with NLTK\n",
    "\n",
    "NLTK has an excellent MaxEnt backed Named Entity Recognizer that is trained on the Penn Treebank. You can also retrain the chunker if you'd like - the code is very readable to extend it with a Gazette or otherwise.([Bengford2016](#Bengford2016))\n",
    "\n",
    "For this example you need to download the NLTK *Averaged Perceptron Tagger* (5.9 Mb) and the NLTK *words* corpus (2.4 Mb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  (PERSON Smith/NNP)\n",
      "  is/VBZ\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  of/IN\n",
      "  (GPE America/NNP)\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP Research/NNP Labs/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"John Smith is from the United States\\\n",
    "                                                    of America and works at Microsoft\\\n",
    "                                                    Research Labs\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with Stanford NER Tagger & NLTK\n",
    "\n",
    "You can also wrap the Stanford NER system, which many of you are also probably used to using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abelma/wordembd/lib/python3.5/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'PERSON'), ('Smith', 'PERSON'), ('is', 'O'), ('from', 'O'), ('the', 'O'), ('United', 'LOCATION'), ('States', 'LOCATION'), ('of', 'LOCATION'), ('America', 'LOCATION'), ('and', 'O'), ('works', 'O'), ('at', 'O'), ('Microsoft', 'ORGANIZATION'), ('Research', 'ORGANIZATION'), ('Labs', 'ORGANIZATION')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "\n",
    "# change the paths below to point to wherever you unzipped the Stanford NER download file\n",
    "stanford_root = '/opt/stanford/stanford-ner-2015-04-20'\n",
    "NER_model = os.path.join(stanford_root, 'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "stanford_jar  = os.path.join(stanford_root, 'stanford-ner-3.5.2.jar')\n",
    "\n",
    "st = StanfordNERTagger(NER_model, stanford_jar, 'utf-8')\n",
    "tuple_list = st.tag(\"John Smith is from the United States of America\\\n",
    "                    and works at Microsoft Research Labs\".split())\n",
    "print(tuple_list)\n",
    "type(tuple_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with FreeLing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*√çndice*](#indice).\n",
    "\n",
    "<a id='references'></a>\n",
    "##Referencias##\n",
    "\n",
    "* <a id='Bengford2016'></a>\n",
    "**[Bengford2016]** Benjaming Bengford, Tony Ojeda, Laura Lorenz, *Natural Language Processing with NLTK and Gensim*, tutorial at PyCon 2016.\n",
    "Notebook *Intro to NLTK*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
