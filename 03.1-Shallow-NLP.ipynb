{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Abel Meneses](http://www.menesesabad.com) for PyData 2018. Source and license info is on [GitHub](https://github.com/sorice/nlp_pydata2018/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bases de NLP\n",
    "\n",
    "[Ver proceso previo: Normalización de Textos basado en Expresiones Regulares](02.3-Text-Normalization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fecha de elaboración inicial**: 8 de agosto de 2015\n",
    "**Última actualización**: \n",
    "<a id='indice'></a>\n",
    "## Índice\n",
    "\n",
    "1. [Datos.](#datos) Descripción de los datos que se utilizarán en el notebook.\n",
    "\n",
    "- 1.1 [Transformar los datos.](#transformar_los_datos) de PDF a txt para hacer más fácil su\n",
    "manipulación en python.\n",
    "\n",
    "2. [Ejemplos Básicos.](#ejemplos_basicos) Usualmente útiles para hacer Minería de Texto.\n",
    "\n",
    "3. [Operaciones usando elementos estadísticos.](#operaciones_estadisticas)\n",
    "\n",
    "- 3.1 [Filtrado de Stopwords](#filtrado_de_stopwords)\n",
    "- 3.2 [Stemming](#stemming)\n",
    "- 3.3 [Lemmatizacion](#lematizacion)\n",
    "- 3.4 [N-gramas](#n-gramas)\n",
    "\n",
    "[Conclusiones](#conclusiones)\n",
    "\n",
    "[Ejercicios](#ejercicios)\n",
    "\n",
    "[Referencias](#referencias)\n",
    "\n",
    "[Índice Alfabético](#indice_alfabetico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='datos'></a>\n",
    "## Datos\n",
    "\n",
    "Para todos los ejemplos se usarán dos libros, uno en inglés y otro en español. El \n",
    "primero es **Free Software Free Society** y el segundo una traducción de este primero\n",
    "**Software Libre para una Sociedad Libre**.\n",
    "\n",
    "<a id='transformar_los_datos'></a>\n",
    "### Transformar los Datos\n",
    "Generalmente casi todos los materiales que poseemos son *PDFs* y para operar con textos\n",
    "en python lo mejor es usar .txt o formatos no enriquecidos. ¿Cómo transformar PDF\n",
    "en TXT?\n",
    "\n",
    "Nuestra recomendación es usar pdftotxt que aparece en los repositorios de GNU/Linux\n",
    "dentro del paquete **poppler-utils**. Si conocen algún escript adecuado para esta tarea\n",
    "utilizando la biblioteca *ghostscript* la recomendamos por encima de *pdftotext*. Sin \n",
    "embargo es bastante difícil encontrar semejante script, no dudamos de que en el futuro\n",
    "lo encontremos. El comando a ejecutar es sencillo:\n",
    "\n",
    "~$ pdftotext archivo.pdf\n",
    "\n",
    "**Resultado:** archivo.txt\n",
    "\n",
    "Hay que tener en cuenta que este script de *extracción de textos* genera los .txt\n",
    "con muchos problemas: fundamentalmente caracteres extraños.\n",
    "Tal vez resultará más útil para el lector estudiar aplicaciones y bibliotecas más \n",
    "especializadas para este tipo de problema del procesamiento de textos como: Apache-Tika,\n",
    "u otros. Sin embargo en el tema anterior sobre pre-procesamiento, el lector podrá\n",
    "encontrar como resolver estos problemas sin usar grandes bibliotecas\n",
    "para un buen trabajo con los ejemplos básicos de NLP que se ofrecen a continuación.\n",
    "\n",
    "<a id='ejemplos_basicos'></a>\n",
    "## Ejemplos básicos al estilo del libro de NLTK\n",
    "\n",
    "Una de las primeras cosas interesantes de un texto, es saber cuantas palabras contiene \n",
    "un texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras:  95976\n"
     ]
    }
   ],
   "source": [
    "#SoftwareWars.txt is generated with pdftotext from SoftwareWars.pdf\n",
    "texto = open('data/SoftwareWars.txt').read()\n",
    "words = 0\n",
    "for line in texto.split('\\n'):\n",
    "    for word in line.split():\n",
    "        words +=1\n",
    "print (\"Total de palabras: \", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras:  102721\n"
     ]
    }
   ],
   "source": [
    "#SoftwareWars2.txt is a normalized version of SoftwareWars.txt\n",
    "texto = open('data/SoftwareWars2.txt').read().lower()\n",
    "words = 0\n",
    "for line in texto.split('\\n'):\n",
    "    for word in line.split():\n",
    "        words +=1\n",
    "print (\"Total de palabras: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un pequeño ejemplo para contar oraciones. _data/Avello2016n_ es una tesis normalizada con el notebook anterior. Este texto de la UCLV ha sido liberado bajo Creative Commons ([Avello2016](#Avello2016)) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032\n"
     ]
    }
   ],
   "source": [
    "tesis = open('data/Avello2016n.txt').read()\n",
    "for line in tesis.split('\\n'):\n",
    "    for i,sentence in enumerate(line.split('.')):\n",
    "        count = i\n",
    "print (count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo con **NLTK**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras:  102721\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(texto)\n",
    "print (\"Total de palabras: \", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos se devuelve el total de palabras(o tokens) divididos por el caracter\n",
    "*espacio*. Sin embargo las palabras en un texto se repiten. ¿Cómo saber las palabras\n",
    "únicas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras únicas: 3082\n"
     ]
    }
   ],
   "source": [
    "tokens_unique=set([])\n",
    "tokens_unique = set(tokens)\n",
    "print (\"Palabras únicas:\", len(tokens_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en el libro Bird et al.[[1](#Bird2009)] la pregunta más simple y \n",
    "común que se hace uno al ver estas cifras suelen ser: ¿Cuál es el promedio de palabras\n",
    "por página? ¿Cuál es la palabra más utilizada? ¿Qué palabras se usan una vez? Veamos\n",
    "algunas formas de calcularlo, para ello necesitaremos algunas funciones extras:\n",
    "\n",
    "<a id='sect2.5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inicializar un diccionario para guardar el # de apariciones de cada palabra.\n",
    "dict = {}\n",
    "for word in tokens_unique:\n",
    "    dict[word]=0\n",
    "#Diccionario con word = # apariciones.\n",
    "for token in tokens:\n",
    "    dict[token]+=1\n",
    "#Operar con una tupla puede ser mejor. Lista([#apariciones,word])\n",
    "tupla = []\n",
    "for word in dict:\n",
    "    tupla.append([dict[word],word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU_07 : 1\n",
      "cinco : 1\n",
      "casos : 1\n",
      "conforman : 1\n",
      "Carly : 1\n"
     ]
    }
   ],
   "source": [
    "#Ver 5 palabras aleatoriamente contenidas en el diccionario.\n",
    "import random\n",
    "for i in random.sample(range(len(dict)),5):\n",
    "    print (tupla[i][1],\":\",tupla[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta tupla puede ser ordenada de la siguiente forma: al poner el # de apariciones \n",
    "delante, podemos usar el elemento *tupla[i][0]* como parámetro de ordenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las 5 palabras más utilizadas son:\n",
      ". : 2032\n",
      "de : 1091\n",
      "la : 578\n",
      "en : 348\n",
      "el : 275\n"
     ]
    }
   ],
   "source": [
    "tupla=sorted(tupla)\n",
    "print (\"Las 5 palabras más utilizadas son:\")\n",
    "for i in range(1,6):\n",
    "    print (tupla[-i][1],\":\",tupla[-i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 palabras que se usan una vez:\n",
      "#m : 1\n",
      "#p : 1\n",
      "00002942 : 1\n",
      "00003133 : 1\n",
      "00078760 : 1\n",
      "Promedio de alabras por página 56.733333333333334\n"
     ]
    }
   ],
   "source": [
    "print (\"5 palabras que se usan una vez:\")\n",
    "for i in range(5):\n",
    "    print (tupla[i][1],\":\",tupla[i][0])\n",
    "#Número de páginas del libro After the Software War = 300\n",
    "print (\"Promedio de palabras por página\", len(tokens)/300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print (dict[\"software\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='operaciones_estadisticas'></a>\n",
    "# Operaciones usando elementos estadísticos\n",
    "\n",
    "<a id='filtrado_de_stopwords'></a>\n",
    "## Filtrado de Stopwords\n",
    "\n",
    "Como habremos podido observar en la respuesta a *\"las palabras más usadas\"*, la mayoría\n",
    "de ellas son palabras que no contienen significado o no son ni verbos, ni sustantivos, \n",
    "ni adjetivos. En NLP se acostumbra a eliminar estas palabras para procesar el resto más\n",
    "significativo. Para hacerlo necesitamos normalmente un fichero de texto con los \n",
    "stopwords que por lo general y por convención se han definido o calculado. \n",
    "En nuestro caso usaremos los de NLTK.Una versión personalizada por el autor también\n",
    "se usa, hecha a partir de los originales de *NLTK* y \n",
    "[Snowball](http://snowball.tartarus.org/texts/introduction.html)\n",
    "\n",
    "**Nota**: otro elemento contenido en el siguiente algoritmo es la eliminación de las \n",
    "palabras con longitud = 1, todas palabras sin significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de filtrado de stopwords:  0.017447471618652344\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "timei = time.time()\n",
    "english_stops = set(stopwords.words('en'))\n",
    "tokens_afterstops=[]\n",
    "for k in range(len(tokens)-1):\n",
    "    if tokens[k] not in english_stops and len(tokens[k])>1:\n",
    "        tokens_afterstops.append(tokens[k])\n",
    "timef = time.time()-timei\n",
    "print (\"Tiempo de filtrado de stopwords: \",timef)\n",
    "\n",
    "tokens_unique1 = set(tokens_afterstops)\n",
    "dict1 = {} #dict con keys = set de tokens after stops\n",
    "for word in tokens_unique1:\n",
    "    dict1[word]=0\n",
    "\n",
    "tupla1 = [] #Creando lista de tuplas(# ocurrencias,word) sin stopwords\n",
    "for token in tokens_afterstops:\n",
    "    dict1[token]+=1\n",
    "for word in dict1:\n",
    "    tupla1.append([dict1[word],word])\n",
    "tupla1=sorted(tupla1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras únicas sin stops: 3002\n",
      "Las 5 palabras más utilizadas después de filtrar los stopwords son:\n",
      "de : 1091\n",
      "la : 578\n",
      "en : 348\n",
      "el : 275\n",
      "se : 213\n",
      "Total de palabras del texto:  17020\n",
      "Palabras en el texto sin stopwords: 13912\n",
      "Palabras eliminadas en el proceso de filtrado de stopwords: 3108\n"
     ]
    }
   ],
   "source": [
    "print (\"Palabras únicas sin stops:\", len(tokens_unique1))\n",
    "print (\"Las 5 palabras más utilizadas después de filtrar los stopwords son:\")\n",
    "for i in range(1,6):\n",
    "    print (tupla1[-i][1],\":\",tupla1[-i][0])\n",
    "print (\"Total de palabras del texto: \", len(tokens))\n",
    "print (\"Palabras en el texto sin stopwords:\", len(tokens_afterstops))\n",
    "print (\"Palabras eliminadas en el proceso de filtrado de stopwords:\", \n",
    "len(tokens)-len(tokens_afterstops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='stemming'></a>\n",
    "## Stemming \n",
    "\n",
    "Proceso mediante el cual se eliminan de la palabra los “morfemas”, utilizando reglas \n",
    "predefinidas que se corresponden con las terminaciones más comunes de las palabras en \n",
    "un idioma. Trabaja la morfología de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de steeming:  0.09572577476501465\n",
      "Ing : Alexand\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words_stops = len(tokens) #Longitud de tokens después de los dos primeros pasos.\n",
    "tokens_stem=[]\n",
    "for i in range(len(tokens_afterstops)):\n",
    "    tokens_stem.append(stemmer.stem(tokens_afterstops[i]))\n",
    "        #print i\n",
    "\n",
    "timei = time.time()\n",
    "stemmer = PorterStemmer()\n",
    "words_stops = len(tokens) #Longitud de tokens después de los dos primeros pasos.\n",
    "tokens_stem=[]\n",
    "for i in range(len(tokens)):\n",
    "    tokens_stem.append(stemmer.stem(tokens[i]))\n",
    "        #print i\n",
    "timef = time.time()-timei\n",
    "print (\"Tiempo de steeming: \",timef)\n",
    "\n",
    "print (tokens_afterstops[27],\":\",tokens_stem[27])\n",
    "tokens_unique2 = set(tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras reducidas durante el steaming: de 3002 se redujeron en 184.\n",
      "Palabras únicas tras el steamming: 2818.\n"
     ]
    }
   ],
   "source": [
    "print (\"Palabras reducidas durante el steaming: de %d se redujeron en %d.\" \n",
    "       % (len(tokens_unique1), len(tokens_unique1)-len(tokens_unique2)))\n",
    "print (\"Palabras únicas tras el steamming: %d.\" % (len(tokens_unique2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='lematizacion'></a>\n",
    "## Lematización\n",
    "\n",
    "Proceso mediante el cual se extrae el “lexema” de la palabra. Generalmente es necesario\n",
    "utilizar una base de datos (BD) que contenga información de los lexemas o lemas (como \n",
    "también se le suele llamar a los lexemas), estas BD son generalmente semánticas. \n",
    "Trabaja la morfología de las palabras.\n",
    "\n",
    "En el siguiente ejemplo se utiliza Wordnet y su implementación en NLTK. **Wordnet** es \n",
    "una de las BD semánticas más importantes creadas por la humanidad cuyas versiones en \n",
    "inglés son licenciadas bajo principios libres. Se puede encontrar varias versiones de ella\n",
    "en los diferentes repositorios de linux. Una versión profesional de Wordnet en español\n",
    "existe pero es comercial y su costo es de más de 5000 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de lematización: 2.6319186687469482\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Lematizando.\n",
    "dict_lem = {}\n",
    "timei=time.time()\n",
    "for i in range(len(tokens)):\n",
    "    dict_lem[tokens[i]] = lemmatizer.lemmatize(tokens[i])\n",
    "timef=time.time()-timei\n",
    "print(\"Tiempo de lematización:\", timef)\n",
    "\n",
    "#Construyendo un diccionario con los términos únicos dict_lem after stops\n",
    "#y una lista con estos mismos términos.\n",
    "\n",
    "dict_lem = {}\n",
    "for token in tokens_unique1:\n",
    "    dict_lem[token] = token\n",
    "tokens_uniqueB = list(dict_lem.keys()) #ojo: en py3 el método keys() devuelve un objeto de tipo dict_keys (no-list) y no indexable.\n",
    "\n",
    "#Lematizando los términos únicos.\n",
    "for i in range(len(tokens_uniqueB)):\n",
    "    dict_lem[tokens_uniqueB[i]] = lemmatizer.lemmatize(tokens_uniqueB[i])\n",
    "\n",
    "tokens_lem = tokens_afterstops.copy()\n",
    "for i in range(len(tokens_afterstops)):\n",
    "    tokens_lem[i] = dict_lem[tokens_afterstops[i]]\n",
    "        \n",
    "tokens_unique3 = set(tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mes : me\n",
      "las : la\n",
      "fines : fine\n",
      "genera : genus\n",
      "las : la\n",
      "morales : morale\n",
      "es : e\n",
      "todas : toda\n",
      "las : la\n",
      "personas : persona\n",
      "Palabras reducidas durante la Lematización de 3002 se redujeron en 17\n",
      "Palabras únicas tras el steamming: 2985\n"
     ]
    }
   ],
   "source": [
    "setDiff = tokens_unique1.union(tokens_unique3) - tokens_unique1.intersection(tokens_unique3)\n",
    "\n",
    "count = 10\n",
    "for i in range(len(tokens_lem)):\n",
    "    if tokens_lem[i] != tokens_afterstops[i]:\n",
    "        print (tokens_afterstops[i],\":\",tokens_lem[i])\n",
    "        count-=1\n",
    "        if count < 1:\n",
    "            break\n",
    "print (\"Palabras reducidas durante la Lematización de %d se redujeron en %d\" \n",
    "       % (len(tokens_unique1), len(tokens_unique1)-len(tokens_unique3)))\n",
    "print (\"Palabras únicas tras el steamming:\", len(tokens_unique3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante destacar que tras el stemming quedaron 2818 palabras y tras la \n",
    "lematización quedaron 2985, en ambos casos se parte del resultado del filtrado\n",
    "de stopwords. En ambos casos también estamos en presencia de una **reducción de la\n",
    "dimensionalidad**, o sea disminuir el tamaño de los datos a analizar perdiendo la menor\n",
    "cantidad de información.\n",
    "Sin embargo el autor recomienda utilizar las 2985 palabras de la lematización porque es\n",
    "un proceso donde se puede regresar a la palabra original, el stemming hace un \n",
    "tronchado de las palabras que es irreversible. Sí hay que notar que la lematización es\n",
    "más lenta, pero tiene en cuenta para generar cada lema la función POS de cada palabra\n",
    "*(o sea sí esta es un verbo, sustantivo, adjetivo o adverbio, una misma palabra puede\n",
    "tener lemas diferentes en función de su POS)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='n-gramas'></a>\n",
    "## N-gramas\n",
    "\n",
    "La técnica de n-gramas es muy simple, consiste en dividir el texto tanto en caracteres\n",
    "como en palabras y hacer nuevos tokens compuestos por n-caracteres o n-palabras. En \n",
    "este ejemplo veremos una división en *n-palabras*. Basadas en las propiedades de **Los\n",
    "Modelos ocultos de Markov** esta nueva lista, para $n \\geqq 3$ suele ser mucho más útil \n",
    "que las palabras simples. La func **ngrams2** fue el segundo diseño elaborado en abril\n",
    "de 2015 para el algoritmo de detección de texto reusado (del autor) en su primera \n",
    "versión. La versión inicial del 2013 se incluye en la sección de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de n-gramas: 102719.\n",
      "N-gramas únicos: 89823.\n"
     ]
    }
   ],
   "source": [
    "def ngrams2(text,n):\n",
    "    tokens=[];ngram=\"\";ngram_list=[]\n",
    "    for word in text.split():\n",
    "        tokens.append(word)\n",
    "    if n >= len(tokens):\n",
    "        raise Exception(\"Not possible, n most be shorter than total words.\")\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        for j in range(i,i+n):\n",
    "            ngram+=tokens[j]+\" \"\n",
    "        ngram_list.append(ngram)\n",
    "        ngram=\"\"\n",
    "    return ngram_list\n",
    "\n",
    "texto = open('data/SoftwareWars2.txt').read()\n",
    "ngramsl=ngrams2(texto,3)\n",
    "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
    "ngramsl_unique = set(ngramsl)\n",
    "print (\"N-gramas únicos: %d.\" % len(ngramsl_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación incluyo una actualización de este algoritmo con conocimientos aprendidos\n",
    "en marzo del 2016 al estudiar high performance in python [[3](#Gorelick2014)]. Implementando códigos \n",
    "pythonicos utilizando estructuras optimizadas (deque) y la función join de la clase str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de n-gramas: 102719.\n",
      "N-gramas únicos: 89823.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def ngrams4(text,n):\n",
    "    tokens=[];ngram_list=deque()\n",
    "    for word in text.split():\n",
    "        tokens.append(word)\n",
    "    if n >= len(tokens):\n",
    "        raise Exception(\"Not possible, n most be shorter than total words.\")\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngram_tokens = [tokens[j] for j in range(i,i+n)]\n",
    "        ngram =\" \".join(ngram_tokens)\n",
    "        ngram_list.append(ngram)\n",
    "    return ngram_list\n",
    "\n",
    "texto = open('data/SoftwareWars2.txt').read()\n",
    "ngramsl=ngrams4(texto,3)\n",
    "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
    "ngramsl_unique = set(ngramsl)\n",
    "print (\"N-gramas únicos: %d.\" % len(ngramsl_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digital download .\n"
     ]
    }
   ],
   "source": [
    "print (ngramsl[26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función ngrams basada en iteradores, el retorno **yield** y las función interna de las listas __iadd__. Elaborado el \n",
    "3 de septiembre de 2016.\n",
    "Como se verá en la sección de [*Profiling*](#profiling) esta implementación es la más rápida de todas(10x) respecto a la\n",
    "original del 2013.\n",
    "La idea de esta función salió de revisar la implementación del método *_split* de la clase **NGram** del módulo \n",
    "[ngram](http://github.com/gpoulter/python-ngram) de Graham Poulter, el mismo es incluído dentro de los recursos \n",
    "de este tema para una revisión más detallada por parte de los estudiantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de n-gramas: 102719.\n",
      "N-gramas únicos: 89823.\n"
     ]
    }
   ],
   "source": [
    "def ngram_split(text,n):\n",
    "    ngram = ''\n",
    "    gram_count = 0\n",
    "    for i,word in enumerate(text.split(),1):\n",
    "        if gram_count-n == -1 and i > n:\n",
    "            ngram = ngram[ngram.find(' ')+1:]\n",
    "        ngram += word+' '; gram_count+=1\n",
    "        if gram_count == n:\n",
    "            gram_count -= 1\n",
    "            yield ngram\n",
    "        \n",
    "def ngrams5(text,n):\n",
    "    ngrams = []\n",
    "    ngrams.__iadd__(ngram_split(text,n))\n",
    "    if len(ngrams) > 0:\n",
    "        return ngrams\n",
    "    else:\n",
    "        raise Exception(\"Not possible, n is longer than total words.\")\n",
    "\n",
    "ngrams = ngrams5(texto,3)\n",
    "print (\"Total de n-gramas: %d.\" % len(ngrams))\n",
    "ngramsl_unique = set(ngramsl)\n",
    "print (\"N-gramas únicos: %d.\" % len(ngramsl_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección [*Profiling*](#profiling) se analiza el performance de estas \n",
    "implementaciones, y el de otras bibliotecas fundamentales de python para NLP. Al \n",
    "concluir se incluye una sección [NLTK ngrams](#nltk_ngrams) para evaluar la dificultad\n",
    "de usar esta función en NLTK.\n",
    "\n",
    "A continuación otra función implementada en el software [Takelab](http://takelab.fer.hr/)\n",
    "un sistema creado en python para el evento SEMEVAL de evaluación semántica de textos. Utiliza también iteradores\n",
    "pero usando la función *zip* y la estructura optimizada *deque*, vista ya en un ejemplo anterior.\n",
    "Esta implementación se estudió y elaboró el 18 de octubre del 2017 durante el desarrollo de los paquetes\n",
    "[preprocess](https://github.com/sorice/preprocess) y del paquete [textsim](https://github.com/sorice/textsim)\n",
    "ambos necesarios para los experimentos de Reconocimiento de Paráfrasis y Detección de Texto Reusado elaborados\n",
    "por el autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de n-gramas: 102719.\n",
      "N-gramas únicos: 89823.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def make_ngrams(l, n):\n",
    "    \"\"\"Ngrams generation func.\"\"\"\n",
    "    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n",
    "    rez.append(l[n - 1:])\n",
    "    return zip(*rez)\n",
    "\n",
    "def ngrams6(text,n):\n",
    "    A = make_ngrams(text.split(),3)\n",
    "    return deque(A)\n",
    "    \n",
    "ngramsl = ngrams6(texto,3)\n",
    "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
    "ngramsl_unique = set(ngramsl)\n",
    "print (\"N-gramas únicos: %d.\" % len(ngramsl_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='profiling'></a>\n",
    "## Profiling\n",
    "\n",
    "Vamos a comparar este método de Abel de n-grams del 2015, el ngram_index del 2013\n",
    "y el método *ngrams* de la biblioteca belga **Patterns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de ngrams nov/2013: 0.8638\n",
      "1 loops, best of 3: 781 ms per loop\n",
      "102719\n",
      "Función de ngrams abril/2015: 0.1403\n",
      "10 loops, best of 3: 123 ms per loop\n",
      "102719\n",
      "Función de ngrams marzo/2016: 0.1565\n",
      "10 loops, best of 3: 145 ms per loop\n",
      "102719\n",
      "Función de ngrams septiembre/2016: 0.1173\n",
      "10 loops, best of 3: 102 ms per loop\n",
      "102719\n",
      "Función de ngrams octubre/2017: 0.1173\n",
      "10 loops, best of 3: 27.7 ms per loop\n",
      "102719\n"
     ]
    }
   ],
   "source": [
    "from time import clock\n",
    "\n",
    "from scripts.tokens import ngram_index as ngrams1\n",
    "start_time2 = clock()\n",
    "ngramsa = ngrams1(texto,3)\n",
    "end_time2 = clock()-start_time2\n",
    "\n",
    "start_time1 = clock()\n",
    "ngramsb = ngrams2(texto,3)\n",
    "end_time1 = clock()-start_time1\n",
    "\n",
    "\"\"\"Esta sección solo se puede usar en py2, pues la \n",
    "biblioteca Pattern no tiene soporte para py3.\"\"\"\n",
    "#from pattern.en import ngrams as ngrams3\n",
    "#start_time3 = clock()\n",
    "#ngramsc = ngrams3(texto,n=3)\n",
    "#end_time3 = clock()-start_time3\n",
    "\n",
    "start_time4 = clock()\n",
    "ngramsd = ngrams4(texto,3)\n",
    "end_time4 = clock()-start_time4\n",
    "\n",
    "start_time5 = clock()\n",
    "ngramse = ngrams5(texto,3)\n",
    "end_time5 = clock()-start_time5\n",
    "\n",
    "start_time6 = clock()\n",
    "ngramsf = ngrams6(texto,3)\n",
    "end_time6 = clock()-start_time6\n",
    "\n",
    "print ('Función de ngrams nov/2013: %.4f' % end_time2)\n",
    "%timeit ngrams1(texto,3)\n",
    "print (len(ngramsb))\n",
    "\n",
    "print ('Función de ngrams abril/2015: %.4f' % end_time1)\n",
    "%timeit ngrams2(texto,3)\n",
    "print (len(ngramsa))\n",
    "\n",
    "#Patter is only active for python2, still working on this\n",
    "#print ('Función de ngrams Pattern octubre/2013: %.4f' % end_time3)\n",
    "#%timeit ngram3(texto,3)\n",
    "#print (len(ngramsc))\n",
    "\n",
    "print ('Función de ngrams marzo/2016: %.4f' % end_time4)\n",
    "%timeit ngrams4(texto,3)\n",
    "print (len(ngramsd))\n",
    "\n",
    "print ('Función de ngrams septiembre/2016: %.4f' % end_time5)\n",
    "%timeit ngrams5(texto,3)\n",
    "print (len(ngramse))\n",
    "\n",
    "print ('Función de ngrams octubre/2017: %.4f' % end_time5)\n",
    "%timeit ngrams6(texto,3)\n",
    "print (len(ngramsf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " digital download .\n",
      "digital download . \n",
      "digital download .\n",
      "digital download . \n",
      "('digital', 'download', '.')\n"
     ]
    }
   ],
   "source": [
    "print (ngramsa[26])\n",
    "print (ngramsb[26])\n",
    "print (ngramsd[26])\n",
    "print (ngramse[26])\n",
    "print (ngramsf[26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nltk_ngrams'></a>\n",
    "## NLTK Ngrams\n",
    "\n",
    "**Nota:** NLTK tiene una función de ngramas\n",
    "     \n",
    "    from nltk import ngrams\n",
    "\n",
    "Sin embargo en la versión 3 esta función es un generador, y como entrada hay que pasar\n",
    "una secuencia de palabras (una lista) y no el texto original. Ejemplo para obtener un resultado similar al que\n",
    "hemos obtenido con las 4 funciones anteriores hacemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de ngrams NLTK octubre/2015: 1.0388\n",
      "1 loops, best of 3: 1 s per loop\n",
      "102719\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams as ngramsNLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "texto = open('SoftwareWars2.txt').read().lower()\n",
    "\n",
    "def ngramsNltk(texto,number):\n",
    "    ngramse =[]\n",
    "    token_text = word_tokenize(texto)\n",
    "    ingramse = ngramsNLTK(token_text,number)\n",
    "    for i in ingramse:\n",
    "        ngramse.append(i)\n",
    "    return ngramse\n",
    "\n",
    "start_time_nltk = clock()\n",
    "ngramsnltk = ngramsNltk(texto,3)\n",
    "end_time_nltk = clock()-start_time_nltk\n",
    "print ('Función de ngrams NLTK octubre/2015: %.4f' % end_time_nltk)\n",
    "%timeit ngramsNltk(texto,3)\n",
    "print (len(ngramsnltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse aquí el resultado es más lento que en cualquiera de los anteriores\n",
    "fundamentalmente por el uso de dos funciones (tokenizar y luego ngramas). En general\n",
    "es más lenta que la mejor solución local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusiones'></a>\n",
    "\n",
    "## Conclusiones\n",
    "- El procesamiento de textos es un mundo fascinante y de enormes utilidades.\n",
    "- En python la biblioteca **NLTK** es un software maduro, aunque con muchas\n",
    "insuficiencias para el idioma español.\n",
    "- La función de ngramas local basada en iteradores, zip y deque es 17x la original del 2013.\n",
    "- **NLTK** puede ser optimizada, sí comparamos casos como la función de ngrams.\n",
    "- Las técnicas fundamentales para descomponer un texto se basan en estadística.\n",
    "- Wordnet y otros recursos como Babelfish suelen dar excelentes resultados para \n",
    "procesamiento de la lengua basado en conocimiento. Sin embargo estos métodos pueden\n",
    "consumir grandes cuotas de HD, CPU y Memoria RAM.\n",
    "- Combinado con leyes de la lingüística como la Ley de Luhn estas técnicas sirven de\n",
    "base para operaciones más complejas dentro de otras áreas de NLP, Minería de Texto e \n",
    "Information Retrieval.\n",
    "- En ocasiones es importante tener presente que para funciones muy utilizadas y simples,\n",
    "podría ser más útil implementar nuestra propia función. Es el ejemplo explicado en la\n",
    "sección de rendimiento y NTLK ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='ejercicios'></a>\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "* **Ejercicio 1:** Implemente un algoritmo que dado un texto cuente las oraciones que hay en él.\n",
    "Pruebe este algoritmo con el texto pre-procesado y sin procesar.\n",
    "* **Ejercicio 2:** Implemente una tokenización de textos en inglés que transforme las contracciones.\n",
    "Cuente el número de palabras únicas con esta implementación y compare con la estudiada\n",
    "en clase.\n",
    "* **Ejercicio 3:** Re-implemente la tokenización estudiada en clases, pero donde el algoritmo sea capaz\n",
    "de entender que *The* y *the* son la misma palabra.\n",
    "* **Ejercicio 4:** Proponga un nuevo conjunto de stopwords, diseñe una ecuación para calcularlos\n",
    "automáticamente. (ver la [Ley de Luhn](03.1b-Ley-de-Luhn.ipynb))\n",
    "* **Ejercicio 5:** Encuentre en internet palabras con raíces linguísticas o lemas poco comunes. Pruebe\n",
    "los resultados con el steeming de NLTK. Rediseñe el algoritmo de Poter para que reduzca\n",
    "bien las palabras encontradas por usted.\n",
    "* **Ejercicio 6:** Proponga una lematización utilizando diccionarios de palabras existentes en internet.\n",
    "* **Ejercicio 7:** Lea el capítulo de máquinas de estado de Jurafsky, e implemente una máquina parecida\n",
    "en python.\n",
    "* **Ejercicio 8:** Utilice los diccionarios de libreoffice de afijos, infijos, etc e implemente un \n",
    "lematizador que utilice este recurso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volver al [*Índice*](#indice).\n",
    "\n",
    "<a id='referencias'></a>\n",
    "## Referencias\n",
    "\n",
    "<a id='Bird2009'></a>\n",
    "[1] _[Bird2009]_ Steven Bird, Ewan Klain & Edward Loper,. \n",
    "Book **Natural Language Processing with Python**. 2009. \n",
    "p. 10 **ISBN**: 978-0-596-51649-9\n",
    "\n",
    "<a id='Luhn1958'></a>\n",
    "[2] _[Luhn1958]_ H.P. Luhn. Paper **The Automatic Creation of Literature Abstract**. \n",
    "*IBM Journal*, 1958.\n",
    "\n",
    "<a id='Gorelick2014'></a>\n",
    "[3] _[Gorelick2014]_ Micha Gorelick & Ian Ozsvald. Book **High Performance Python**. \n",
    "O'Reilly. 2014. **ISBN**: 978-1-449-36159-4\n",
    "\n",
    "<a id='Avello2016'></a>\n",
    "[3] _[Avello2016]_ Alexander Avello. Thesis **Módulo QtNLP_Wordnet para la aplicación QtNLP**. \n",
    "UCLV. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='alphabetic_index'></a>\n",
    "## Índice Alfabético\n",
    "\n",
    "<a id='token'></a>\n",
    "**Token**: señal, indicio, muestra. Se usa generalmente para referirse a la unidad\n",
    "más pequeña de procesamiento: palabras, fonemas, n-grams, etc..\n",
    "\n",
    "\n",
    "Volver al [*Índice*](#indice)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
