{
 "metadata": {
  "name": "",
  "signature": "sha256:d70350f0ae93a3f44f7974bdfa393f32857f046b48fe83293f795bdea12af22d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Abel Meneses](http://www.menesesabad.com) for PyData 2018. Source and license info is on [GitHub](https://github.com/sorice/nlp_pydata2018/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Bases de NLP#\n",
      "\n",
      "[Ver proceso previo: Normalizaci\u00f3n de Textos basado en Expresiones Regulares](02.3-Text-Normalization.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Fecha de elaboraci\u00f3n inicial**: 8 de agosto de 2015\n",
      "**\u00daltima actualizaci\u00f3n**: \n",
      "<a id='indice'></a>\n",
      "##\u00cdndice##\n",
      "\n",
      "1. [Datos.](#datos) Descripci\u00f3n de los datos que se utilizar\u00e1n en el notebook.\n",
      "\n",
      "- 1.1 [Transformar los datos.](#transformar_los_datos) de PDF a txt para hacer m\u00e1s f\u00e1cil su\n",
      "manipulaci\u00f3n en python.\n",
      "\n",
      "2. [Ejemplos B\u00e1sicos.](#ejemplos_basicos) Usualmente \u00fatiles para hacer Miner\u00eda de Texto.\n",
      "\n",
      "3. [Operaciones usando elementos estad\u00edsticos.](#operaciones_estadisticas)\n",
      "\n",
      "- 3.1 [Filtrado de Stopwords](#filtrado_de_stopwords)\n",
      "- 3.2 [Stemming](#stemming)\n",
      "- 3.3 [Lemmatizacion](#lematizacion)\n",
      "- 3.4 [N-gramas](#n-gramas)\n",
      "\n",
      "[Conclusiones](#conclusiones)\n",
      "\n",
      "[Ejercicios](#ejercicios)\n",
      "\n",
      "[Referencias](#referencias)\n",
      "\n",
      "[\u00cdndice Alfab\u00e9tico](#indice_alfabetico)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='datos'></a>\n",
      "##Datos##\n",
      "\n",
      "Para todos los ejemplos se usar\u00e1n dos libros, uno en ingl\u00e9s y otro en espa\u00f1ol. El \n",
      "primero es **Free Software Free Society** y el segundo una traducci\u00f3n de este primero\n",
      "**Software Libre para una Sociedad Libre**.\n",
      "\n",
      "<a id='transformar_los_datos'></a>\n",
      "###Transformar los datos###\n",
      "Generalmente casi todos los materiales que poseemos son *PDFs* y para operar con textos\n",
      "en python lo mejor es usar .txt o formatos no enriquecidos. \u00bfC\u00f3mo transformar PDF\n",
      "en TXT?\n",
      "\n",
      "Nuestra recomendaci\u00f3n es usar pdftotxt que aparece en los repositorios de GNU/Linux\n",
      "dentro del paquete **poppler-utils**. Si conocen alg\u00fan escript adecuado para esta tarea\n",
      "utilizando la biblioteca *ghostscript* la recomendamos por encima de *pdftotext*. Sin \n",
      "embargo es bastante dif\u00edcil encontrar semejante script, no dudamos de que en el futuro\n",
      "lo encontremos. El comando a ejecutar es sencillo:\n",
      "\n",
      "~$ pdftotext archivo.pdf\n",
      "\n",
      "**Resultado:** archivo.txt\n",
      "\n",
      "Hay que tener en cuenta que este script de *extracci\u00f3n de textos* genera los .txt\n",
      "con muchos problemas: fundamentalmente caracteres extra\u00f1os.\n",
      "Tal vez resultar\u00e1 m\u00e1s \u00fatil para el lector estudiar aplicaciones y bibliotecas m\u00e1s \n",
      "especializadas para este tipo de problema del procesamiento de textos como: Apache-Tika,\n",
      "u otros. Sin embargo en el tema anterior sobre pre-procesamiento, el lector podr\u00e1\n",
      "encontrar como resolver estos problemas sin usar grandes bibliotecas\n",
      "para un buen trabajo con los ejemplos b\u00e1sicos de NLP que se ofrecen a continuaci\u00f3n.\n",
      "\n",
      "<a id='ejemplos_basicos'></a>\n",
      "##Ejemplos b\u00e1sicos al estilo del libro de NLTK##\n",
      "\n",
      "Una de las primeras cosas interesantes de un texto, es saber cuantas palabras contiene \n",
      "un texto."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SoftwareWars.txt is generated with pdftotext from SoftwareWars.pdf\n",
      "texto = open('data/SoftwareWars.txt').read()\n",
      "words = 0\n",
      "for line in texto.split('\\n'):\n",
      "    for word in line.split():\n",
      "        words +=1\n",
      "print (\"Total de palabras: \", words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de palabras:  95976\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SoftwareWars2.txt is a normalized version of SoftwareWars.txt\n",
      "texto = open('data/SoftwareWars2.txt').read().lower()\n",
      "words = 0\n",
      "for line in texto.split('\\n'):\n",
      "    for word in line.split():\n",
      "        words +=1\n",
      "print (\"Total de palabras: \", words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de palabras:  102721\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Un peque\u00f1o ejemplo para contar oraciones. test2.txt es una tesis ([Avello2016](#Avello2016)) normalizada con el notebook anterior."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tesis = open('data/Avello2016n.txt').read()\n",
      "for line in tesis.split('\\n'):\n",
      "    for i,sentence in enumerate(line.split('.')):\n",
      "        count = i\n",
      "print (count)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2032\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ejemplo con **NLTK**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
      "tokens = tokenizer.tokenize(texto)\n",
      "print (\"Total de palabras: \", len(tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de palabras:  102721\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En ambos casos se devuelve el total de palabras(o tokens) divididos por el caracter\n",
      "*espacio*. Sin embargo las palabras en un texto se repiten. \u00bfC\u00f3mo saber las palabras\n",
      "\u00fanicas?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens_unique=set([])\n",
      "tokens_unique = set(tokens)\n",
      "print (\"Palabras \u00fanicas:\", len(tokens_unique))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Palabras \u00fanicas: 3082\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como se puede ver en el libro Bird et al.[[1](#Bird2009)] la pregunta m\u00e1s simple y \n",
      "com\u00fan que se hace uno al ver estas cifras suelen ser: \u00bfCu\u00e1l es el promedio de palabras\n",
      "por p\u00e1gina? \u00bfCu\u00e1l es la palabra m\u00e1s utilizada? \u00bfQu\u00e9 palabras se usan una vez? Veamos\n",
      "algunas formas de calcularlo, para ello necesitaremos algunas funciones extras:\n",
      "\n",
      "<a id='sect2.5'></a>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#Inicializar un diccionario para guardar el # de apariciones de cada palabra.\n",
      "dict = {}\n",
      "for word in tokens_unique:\n",
      "    dict[word]=0\n",
      "#Diccionario con word = # apariciones.\n",
      "for token in tokens:\n",
      "    dict[token]+=1\n",
      "#Operar con una tupla puede ser mejor. Lista([#apariciones,word])\n",
      "tupla = []\n",
      "for word in dict:\n",
      "    tupla.append([dict[word],word])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Ver 5 palabras aleatoriamente contenidas en el diccionario.\n",
      "import random\n",
      "for i in random.sample(range(len(dict)),5):\n",
      "    print (tupla[i][1],\":\",tupla[i][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "HU_07 : 1\n",
        "cinco : 1\n",
        "casos : 1\n",
        "conforman : 1\n",
        "Carly : 1\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Esta tupla puede ser ordenada de la siguiente forma: al poner el # de apariciones \n",
      "delante, podemos usar el elemento *tupla[i][0]* como par\u00e1metro de ordenamiento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tupla=sorted(tupla)\n",
      "print (\"Las 5 palabras m\u00e1s utilizadas son:\")\n",
      "for i in range(1,6):\n",
      "    print (tupla[-i][1],\":\",tupla[-i][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Las 5 palabras m\u00e1s utilizadas son:\n",
        ". : 2032\n",
        "de : 1091\n",
        "la : 578\n",
        "en : 348\n",
        "el : 275\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (\"5 palabras que se usan una vez:\")\n",
      "for i in range(5):\n",
      "    print (tupla[i][1],\":\",tupla[i][0])\n",
      "#N\u00famero de p\u00e1ginas del libro After the Software War = 300\n",
      "print (\"Promedio de palabras por p\u00e1gina\", len(tokens)/300)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5 palabras que se usan una vez:\n",
        "#m : 1\n",
        "#p : 1\n",
        "00002942 : 1\n",
        "00003133 : 1\n",
        "00078760 : 1\n",
        "Promedio de alabras por p\u00e1gina 56.733333333333334\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (dict[\"software\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "19\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='operaciones_estadisticas'></a>\n",
      "#Operaciones usando elementos estad\u00edsticos#\n",
      "\n",
      "<a id='filtrado_de_stopwords'></a>\n",
      "##Filtrado de Stopwords##\n",
      "\n",
      "Como habremos podido observar en la respuesta a *\"las palabras m\u00e1s usadas\"*, la mayor\u00eda\n",
      "de ellas son palabras que no contienen significado o no son ni verbos, ni sustantivos, \n",
      "ni adjetivos. En NLP se acostumbra a eliminar estas palabras para procesar el resto m\u00e1s\n",
      "significativo. Para hacerlo necesitamos normalmente un fichero de texto con los \n",
      "stopwords que por lo general y por convenci\u00f3n se han definido o calculado. \n",
      "En nuestro caso usaremos los de NLTK.Una versi\u00f3n personalizada por el autor tambi\u00e9n\n",
      "se usa, hecha a partir de los originales de *NLTK* y \n",
      "[Snowball](http://snowball.tartarus.org/texts/introduction.html)\n",
      "\n",
      "**Nota**: otro elemento contenido en el siguiente algoritmo es la eliminaci\u00f3n de las \n",
      "palabras con longitud = 1, todas palabras sin significado."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import time\n",
      "from nltk.corpus import stopwords\n",
      "timei = time.time()\n",
      "english_stops = set(stopwords.words('en'))\n",
      "tokens_afterstops=[]\n",
      "for k in range(len(tokens)-1):\n",
      "    if tokens[k] not in english_stops and len(tokens[k])>1:\n",
      "        tokens_afterstops.append(tokens[k])\n",
      "timef = time.time()-timei\n",
      "print (\"Tiempo de filtrado de stopwords: \",timef)\n",
      "\n",
      "tokens_unique1 = set(tokens_afterstops)\n",
      "dict1 = {} #dict con keys = set de tokens after stops\n",
      "for word in tokens_unique1:\n",
      "    dict1[word]=0\n",
      "\n",
      "tupla1 = [] #Creando lista de tuplas(# ocurrencias,word) sin stopwords\n",
      "for token in tokens_afterstops:\n",
      "    dict1[token]+=1\n",
      "for word in dict1:\n",
      "    tupla1.append([dict1[word],word])\n",
      "tupla1=sorted(tupla1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tiempo de filtrado de stopwords:  0.017447471618652344\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (\"Palabras \u00fanicas sin stops:\", len(tokens_unique1))\n",
      "print (\"Las 5 palabras m\u00e1s utilizadas despu\u00e9s de filtrar los stopwords son:\")\n",
      "for i in range(1,6):\n",
      "    print (tupla1[-i][1],\":\",tupla1[-i][0])\n",
      "print (\"Total de palabras del texto: \", len(tokens))\n",
      "print (\"Palabras en el texto sin stopwords:\", len(tokens_afterstops))\n",
      "print (\"Palabras eliminadas en el proceso de filtrado de stopwords:\", \n",
      "len(tokens)-len(tokens_afterstops))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Palabras \u00fanicas sin stops: 3002\n",
        "Las 5 palabras m\u00e1s utilizadas despu\u00e9s de filtrar los stopwords son:\n",
        "de : 1091\n",
        "la : 578\n",
        "en : 348\n",
        "el : 275\n",
        "se : 213\n",
        "Total de palabras del texto:  17020\n",
        "Palabras en el texto sin stopwords: 13912\n",
        "Palabras eliminadas en el proceso de filtrado de stopwords: 3108\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='stemming'></a>\n",
      "##Stemming##\n",
      "\n",
      "Proceso mediante el cual se eliminan de la palabra los \u201cmorfemas\u201d, utilizando reglas \n",
      "predefinidas que se corresponden con las terminaciones m\u00e1s comunes de las palabras en \n",
      "un idioma. Trabaja la morfolog\u00eda de las palabras."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "words_stops = len(tokens) #Longitud de tokens despu\u00e9s de los dos primeros pasos.\n",
      "tokens_stem=[]\n",
      "for i in range(len(tokens_afterstops)):\n",
      "    tokens_stem.append(stemmer.stem(tokens_afterstops[i]))\n",
      "        #print i\n",
      "\n",
      "timei = time.time()\n",
      "stemmer = PorterStemmer()\n",
      "words_stops = len(tokens) #Longitud de tokens despu\u00e9s de los dos primeros pasos.\n",
      "tokens_stem=[]\n",
      "for i in range(len(tokens)):\n",
      "    tokens_stem.append(stemmer.stem(tokens[i]))\n",
      "        #print i\n",
      "timef = time.time()-timei\n",
      "print (\"Tiempo de steeming: \",timef)\n",
      "\n",
      "print (tokens_afterstops[27],\":\",tokens_stem[27])\n",
      "tokens_unique2 = set(tokens_stem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tiempo de steeming:  0.09572577476501465\n",
        "Ing : Alexand\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (\"Palabras reducidas durante el steaming: de %d se redujeron en %d.\" \n",
      "       % (len(tokens_unique1), len(tokens_unique1)-len(tokens_unique2)))\n",
      "print (\"Palabras \u00fanicas tras el steamming: %d.\" % (len(tokens_unique2)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Palabras reducidas durante el steaming: de 3002 se redujeron en 184.\n",
        "Palabras \u00fanicas tras el steamming: 2818.\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='lematizacion'></a>\n",
      "##Lematizaci\u00f3n##\n",
      "\n",
      "Proceso mediante el cual se extrae el \u201clexema\u201d de la palabra. Generalmente es necesario\n",
      "utilizar una base de datos (BD) que contenga informaci\u00f3n de los lexemas o lemas (como \n",
      "tambi\u00e9n se le suele llamar a los lexemas), estas BD son generalmente sem\u00e1nticas. \n",
      "Trabaja la morfolog\u00eda de las palabras.\n",
      "\n",
      "En el siguiente ejemplo se utiliza Wordnet y su implementaci\u00f3n en NLTK. **Wordnet** es \n",
      "una de las BD sem\u00e1nticas m\u00e1s importantes creadas por la humanidad cuyas versiones en \n",
      "ingl\u00e9s son licenciadas bajo principios libres. Se puede encontrar varias versiones de ella\n",
      "en los diferentes repositorios de linux. Una versi\u00f3n profesional de Wordnet en espa\u00f1ol\n",
      "existe pero es comercial y su costo es de m\u00e1s de 5000 euros."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.stem import WordNetLemmatizer\n",
      "lemmatizer = WordNetLemmatizer()\n",
      "\n",
      "#Lematizando.\n",
      "dict_lem = {}\n",
      "timei=time.time()\n",
      "for i in range(len(tokens)):\n",
      "    dict_lem[tokens[i]] = lemmatizer.lemmatize(tokens[i])\n",
      "timef=time.time()-timei\n",
      "print(\"Tiempo de lematizaci\u00f3n:\", timef)\n",
      "\n",
      "#Construyendo un diccionario con los t\u00e9rminos \u00fanicos dict_lem after stops\n",
      "#y una lista con estos mismos t\u00e9rminos.\n",
      "\n",
      "dict_lem = {}\n",
      "for token in tokens_unique1:\n",
      "    dict_lem[token] = token\n",
      "tokens_uniqueB = list(dict_lem.keys()) #ojo: en py3 el m\u00e9todo keys() devuelve un objeto de tipo dict_keys (no-list) y no indexable.\n",
      "\n",
      "#Lematizando los t\u00e9rminos \u00fanicos.\n",
      "for i in range(len(tokens_uniqueB)):\n",
      "    dict_lem[tokens_uniqueB[i]] = lemmatizer.lemmatize(tokens_uniqueB[i])\n",
      "\n",
      "tokens_lem = tokens_afterstops.copy()\n",
      "for i in range(len(tokens_afterstops)):\n",
      "    tokens_lem[i] = dict_lem[tokens_afterstops[i]]\n",
      "        \n",
      "tokens_unique3 = set(tokens_lem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Tiempo de lematizaci\u00f3n: 2.6319186687469482\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "setDiff = tokens_unique1.union(tokens_unique3) - tokens_unique1.intersection(tokens_unique3)\n",
      "\n",
      "count = 10\n",
      "for i in range(len(tokens_lem)):\n",
      "    if tokens_lem[i] != tokens_afterstops[i]:\n",
      "        print (tokens_afterstops[i],\":\",tokens_lem[i])\n",
      "        count-=1\n",
      "        if count < 1:\n",
      "            break\n",
      "print (\"Palabras reducidas durante la Lematizaci\u00f3n de %d se redujeron en %d\" \n",
      "       % (len(tokens_unique1), len(tokens_unique1)-len(tokens_unique3)))\n",
      "print (\"Palabras \u00fanicas tras el steamming:\", len(tokens_unique3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "mes : me\n",
        "las : la\n",
        "fines : fine\n",
        "genera : genus\n",
        "las : la\n",
        "morales : morale\n",
        "es : e\n",
        "todas : toda\n",
        "las : la\n",
        "personas : persona\n",
        "Palabras reducidas durante la Lematizaci\u00f3n de 3002 se redujeron en 17\n",
        "Palabras \u00fanicas tras el steamming: 2985\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Es importante destacar que tras el stemming quedaron 2818 palabras y tras la \n",
      "lematizaci\u00f3n quedaron 2985, en ambos casos se parte del resultado del filtrado\n",
      "de stopwords. En ambos casos tambi\u00e9n estamos en presencia de una **reducci\u00f3n de la\n",
      "dimensionalidad**, o sea disminuir el tama\u00f1o de los datos a analizar perdiendo la menor\n",
      "cantidad de informaci\u00f3n.\n",
      "Sin embargo el autor recomienda utilizar las 2985 palabras de la lematizaci\u00f3n porque es\n",
      "un proceso donde se puede regresar a la palabra original, el stemming hace un \n",
      "tronchado de las palabras que es irreversible. S\u00ed hay que notar que la lematizaci\u00f3n es\n",
      "m\u00e1s lenta, pero tiene en cuenta para generar cada lema la funci\u00f3n POS de cada palabra\n",
      "*(o sea s\u00ed esta es un verbo, sustantivo, adjetivo o adverbio, una misma palabra puede\n",
      "tener lemas diferentes en funci\u00f3n de su POS)*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='n-gramas'></a>\n",
      "##N-gramas##\n",
      "\n",
      "La t\u00e9cnica de n-gramas es muy simple, consiste en dividir el texto tanto en caracteres\n",
      "como en palabras y hacer nuevos tokens compuestos por n-caracteres o n-palabras. En \n",
      "este ejemplo veremos una divisi\u00f3n en *n-palabras*. Basadas en las propiedades de **Los\n",
      "Modelos ocultos de Markov** esta nueva lista, para $n \\geqq 3$ suele ser mucho m\u00e1s \u00fatil \n",
      "que las palabras simples. La func **ngrams2** fue el segundo dise\u00f1o elaborado en abril\n",
      "de 2015 para el algoritmo de detecci\u00f3n de texto reusado (del autor) en su primera \n",
      "versi\u00f3n. La versi\u00f3n inicial del 2013 se incluye en la secci\u00f3n de performance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrams2(text,n):\n",
      "    tokens=[];ngram=\"\";ngram_list=[]\n",
      "    for word in text.split():\n",
      "        tokens.append(word)\n",
      "    if n >= len(tokens):\n",
      "        raise Exception(\"Not possible, n most be shorter than total words.\")\n",
      "    for i in range(len(tokens)-n+1):\n",
      "        for j in range(i,i+n):\n",
      "            ngram+=tokens[j]+\" \"\n",
      "        ngram_list.append(ngram)\n",
      "        ngram=\"\"\n",
      "    return ngram_list\n",
      "\n",
      "texto = open('data/SoftwareWars2.txt').read()\n",
      "ngramsl=ngrams2(texto,3)\n",
      "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
      "ngramsl_unique = set(ngramsl)\n",
      "print (\"N-gramas \u00fanicos: %d.\" % len(ngramsl_unique))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de n-gramas: 102719.\n",
        "N-gramas \u00fanicos: 89823.\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n incluyo una actualizaci\u00f3n de este algoritmo con conocimientos aprendidos\n",
      "en marzo del 2016 al estudiar high performance in python [[3](#Gorelick2014)]. Implementando c\u00f3digos \n",
      "pythonicos utilizando estructuras optimizadas (deque) y la funci\u00f3n join de la clase str."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import deque\n",
      "\n",
      "def ngrams4(text,n):\n",
      "    tokens=[];ngram_list=deque()\n",
      "    for word in text.split():\n",
      "        tokens.append(word)\n",
      "    if n >= len(tokens):\n",
      "        raise Exception(\"Not possible, n most be shorter than total words.\")\n",
      "    for i in range(len(tokens)-n+1):\n",
      "        ngram_tokens = [tokens[j] for j in range(i,i+n)]\n",
      "        ngram =\" \".join(ngram_tokens)\n",
      "        ngram_list.append(ngram)\n",
      "    return ngram_list\n",
      "\n",
      "texto = open('data/SoftwareWars2.txt').read()\n",
      "ngramsl=ngrams4(texto,3)\n",
      "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
      "ngramsl_unique = set(ngramsl)\n",
      "print (\"N-gramas \u00fanicos: %d.\" % len(ngramsl_unique))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de n-gramas: 102719.\n",
        "N-gramas \u00fanicos: 89823.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (ngramsl[26])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "digital download .\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Funci\u00f3n ngrams basada en iteradores, el retorno **yield** y las funci\u00f3n interna de las listas __iadd__. Elaborado el \n",
      "3 de septiembre de 2016.\n",
      "Como se ver\u00e1 en la secci\u00f3n de [*Profiling*](#profiling) esta implementaci\u00f3n es la m\u00e1s r\u00e1pida de todas(10x) respecto a la\n",
      "original del 2013.\n",
      "La idea de esta funci\u00f3n sali\u00f3 de revisar la implementaci\u00f3n del m\u00e9todo *_split* de la clase **NGram** del m\u00f3dulo \n",
      "[ngram](http://github.com/gpoulter/python-ngram) de Graham Poulter, el mismo es inclu\u00eddo dentro de los recursos \n",
      "de este tema para una revisi\u00f3n m\u00e1s detallada por parte de los estudiantes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngram_split(text,n):\n",
      "    ngram = ''\n",
      "    gram_count = 0\n",
      "    for i,word in enumerate(text.split(),1):\n",
      "        if gram_count-n == -1 and i > n:\n",
      "            ngram = ngram[ngram.find(' ')+1:]\n",
      "        ngram += word+' '; gram_count+=1\n",
      "        if gram_count == n:\n",
      "            gram_count -= 1\n",
      "            yield ngram\n",
      "        \n",
      "def ngrams5(text,n):\n",
      "    ngrams = []\n",
      "    ngrams.__iadd__(ngram_split(text,n))\n",
      "    if len(ngrams) > 0:\n",
      "        return ngrams\n",
      "    else:\n",
      "        raise Exception(\"Not possible, n is longer than total words.\")\n",
      "\n",
      "ngrams = ngrams5(texto,3)\n",
      "print (\"Total de n-gramas: %d.\" % len(ngrams))\n",
      "ngramsl_unique = set(ngramsl)\n",
      "print (\"N-gramas \u00fanicos: %d.\" % len(ngramsl_unique))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de n-gramas: 102719.\n",
        "N-gramas \u00fanicos: 89823.\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En la secci\u00f3n [*Profiling*](#profiling) se analiza el performance de estas \n",
      "implementaciones, y el de otras bibliotecas fundamentales de python para NLP. Al \n",
      "concluir se incluye una secci\u00f3n [NLTK ngrams](#nltk_ngrams) para evaluar la dificultad\n",
      "de usar esta funci\u00f3n en NLTK.\n",
      "\n",
      "A continuaci\u00f3n otra funci\u00f3n implementada en el software [Takelab](http://takelab.fer.hr/)\n",
      "un sistema creado en python para el evento SEMEVAL de evaluaci\u00f3n sem\u00e1ntica de textos. Utiliza tambi\u00e9n iteradores\n",
      "pero usando la funci\u00f3n *zip* y la estructura optimizada *deque*, vista ya en un ejemplo anterior.\n",
      "Esta implementaci\u00f3n se estudi\u00f3 y elabor\u00f3 el 18 de octubre del 2017 durante el desarrollo de los paquetes\n",
      "[preprocess](https://github.com/sorice/preprocess) y del paquete [textsim](https://github.com/sorice/textsim)\n",
      "ambos necesarios para los experimentos de Reconocimiento de Par\u00e1frasis y Detecci\u00f3n de Texto Reusado elaborados\n",
      "por el autor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import deque\n",
      "\n",
      "def make_ngrams(l, n):\n",
      "    \"\"\"Ngrams generation func.\"\"\"\n",
      "    rez = [l[i:(-n + i + 1)] for i in range(n - 1)]\n",
      "    rez.append(l[n - 1:])\n",
      "    return zip(*rez)\n",
      "\n",
      "def ngrams6(text,n):\n",
      "    A = make_ngrams(text.split(),3)\n",
      "    return deque(A)\n",
      "    \n",
      "ngramsl = ngrams6(texto,3)\n",
      "print (\"Total de n-gramas: %d.\" % len(ngramsl))\n",
      "ngramsl_unique = set(ngramsl)\n",
      "print (\"N-gramas \u00fanicos: %d.\" % len(ngramsl_unique))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total de n-gramas: 102719.\n",
        "N-gramas \u00fanicos: 89823.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='profiling'></a>\n",
      "##Profiling##\n",
      "\n",
      "Vamos a comparar este m\u00e9todo de Abel de n-grams del 2015, el ngram_index del 2013\n",
      "y el m\u00e9todo *ngrams* de la biblioteca belga **Patterns**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import clock\n",
      "\n",
      "from scripts.tokens import ngram_index as ngrams1\n",
      "start_time2 = clock()\n",
      "ngramsa = ngrams1(texto,3)\n",
      "end_time2 = clock()-start_time2\n",
      "\n",
      "start_time1 = clock()\n",
      "ngramsb = ngrams2(texto,3)\n",
      "end_time1 = clock()-start_time1\n",
      "\n",
      "\"\"\"Esta secci\u00f3n solo se puede usar en py2, pues la \n",
      "biblioteca Pattern no tiene soporte para py3.\"\"\"\n",
      "#from pattern.en import ngrams as ngrams3\n",
      "#start_time3 = clock()\n",
      "#ngramsc = ngrams3(texto,n=3)\n",
      "#end_time3 = clock()-start_time3\n",
      "\n",
      "start_time4 = clock()\n",
      "ngramsd = ngrams4(texto,3)\n",
      "end_time4 = clock()-start_time4\n",
      "\n",
      "start_time5 = clock()\n",
      "ngramse = ngrams5(texto,3)\n",
      "end_time5 = clock()-start_time5\n",
      "\n",
      "start_time6 = clock()\n",
      "ngramsf = ngrams6(texto,3)\n",
      "end_time6 = clock()-start_time6\n",
      "\n",
      "print ('Funci\u00f3n de ngrams nov/2013: %.4f' % end_time2)\n",
      "%timeit ngrams1(texto,3)\n",
      "print (len(ngramsb))\n",
      "\n",
      "print ('Funci\u00f3n de ngrams abril/2015: %.4f' % end_time1)\n",
      "%timeit ngrams2(texto,3)\n",
      "print (len(ngramsa))\n",
      "\n",
      "#Patter is only active for python2, still working on this\n",
      "#print ('Funci\u00f3n de ngrams Pattern octubre/2013: %.4f' % end_time3)\n",
      "#%timeit ngram3(texto,3)\n",
      "#print (len(ngramsc))\n",
      "\n",
      "print ('Funci\u00f3n de ngrams marzo/2016: %.4f' % end_time4)\n",
      "%timeit ngrams4(texto,3)\n",
      "print (len(ngramsd))\n",
      "\n",
      "print ('Funci\u00f3n de ngrams septiembre/2016: %.4f' % end_time5)\n",
      "%timeit ngrams5(texto,3)\n",
      "print (len(ngramse))\n",
      "\n",
      "print ('Funci\u00f3n de ngrams octubre/2017: %.4f' % end_time5)\n",
      "%timeit ngrams6(texto,3)\n",
      "print (len(ngramsf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Funci\u00f3n de ngrams nov/2013: 0.8638\n",
        "1 loops, best of 3: 781 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n",
        "Funci\u00f3n de ngrams abril/2015: 0.1403\n",
        "10 loops, best of 3: 123 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n",
        "Funci\u00f3n de ngrams marzo/2016: 0.1565\n",
        "10 loops, best of 3: 145 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n",
        "Funci\u00f3n de ngrams septiembre/2016: 0.1173\n",
        "10 loops, best of 3: 102 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n",
        "Funci\u00f3n de ngrams octubre/2017: 0.1173\n",
        "10 loops, best of 3: 27.7 ms per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (ngramsa[26])\n",
      "print (ngramsb[26])\n",
      "print (ngramsd[26])\n",
      "print (ngramse[26])\n",
      "print (ngramsf[26])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " digital download .\n",
        "digital download . \n",
        "digital download .\n",
        "digital download . \n",
        "('digital', 'download', '.')\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='nltk_ngrams'></a>\n",
      "##NLTK Ngrams##\n",
      "\n",
      "**Nota:** NLTK tiene una funci\u00f3n de ngramas\n",
      "     \n",
      "    from nltk import ngrams\n",
      "\n",
      "Sin embargo en la versi\u00f3n 3 esta funci\u00f3n es un generador, y como entrada hay que pasar\n",
      "una secuencia de palabras (una lista) y no el texto original. Ejemplo para obtener un resultado similar al que\n",
      "hemos obtenido con las 4 funciones anteriores hacemos lo siguiente:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import ngrams as ngramsNLTK\n",
      "from nltk.tokenize import word_tokenize\n",
      "texto = open('SoftwareWars2.txt').read().lower()\n",
      "\n",
      "def ngramsNltk(texto,number):\n",
      "    ngramse =[]\n",
      "    token_text = word_tokenize(texto)\n",
      "    ingramse = ngramsNLTK(token_text,number)\n",
      "    for i in ingramse:\n",
      "        ngramse.append(i)\n",
      "    return ngramse\n",
      "\n",
      "start_time_nltk = clock()\n",
      "ngramsnltk = ngramsNltk(texto,3)\n",
      "end_time_nltk = clock()-start_time_nltk\n",
      "print ('Funci\u00f3n de ngrams NLTK octubre/2015: %.4f' % end_time_nltk)\n",
      "%timeit ngramsNltk(texto,3)\n",
      "print (len(ngramsnltk))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Funci\u00f3n de ngrams NLTK octubre/2015: 1.0388\n",
        "1 loops, best of 3: 1 s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "102719\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como puede observarse aqu\u00ed el resultado es m\u00e1s lento que en cualquiera de los anteriores\n",
      "fundamentalmente por el uso de dos funciones (tokenizar y luego ngramas). En general\n",
      "es m\u00e1s lenta que la mejor soluci\u00f3n local."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='conclusiones'></a>\n",
      "\n",
      "##Conclusiones##\n",
      "- El procesamiento de textos es un mundo fascinante y de enormes utilidades.\n",
      "- En python la biblioteca **NLTK** es un software maduro, aunque con muchas\n",
      "insuficiencias para el idioma espa\u00f1ol.\n",
      "- La funci\u00f3n de ngramas local basada en iteradores, zip y deque es 17x la original del 2013.\n",
      "- **NLTK** puede ser optimizada, s\u00ed comparamos casos como la funci\u00f3n de ngrams.\n",
      "- Las t\u00e9cnicas fundamentales para descomponer un texto se basan en estad\u00edstica.\n",
      "- Wordnet y otros recursos como Babelfish suelen dar excelentes resultados para \n",
      "procesamiento de la lengua basado en conocimiento. Sin embargo estos m\u00e9todos pueden\n",
      "consumir grandes cuotas de HD, CPU y Memoria RAM.\n",
      "- Combinado con leyes de la ling\u00fc\u00edstica como la Ley de Luhn estas t\u00e9cnicas sirven de\n",
      "base para operaciones m\u00e1s complejas dentro de otras \u00e1reas de NLP, Miner\u00eda de Texto e \n",
      "Information Retrieval.\n",
      "- En ocasiones es importante tener presente que para funciones muy utilizadas y simples,\n",
      "podr\u00eda ser m\u00e1s \u00fatil implementar nuestra propia funci\u00f3n. Es el ejemplo explicado en la\n",
      "secci\u00f3n de rendimiento y NTLK ngrams."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='ejercicios'></a>\n",
      "\n",
      "##Ejercicios##\n",
      "\n",
      "* **Ejercicio 1:** Implemente un algoritmo que dado un texto cuente las oraciones que hay en \u00e9l.\n",
      "Pruebe este algoritmo con el texto pre-procesado y sin procesar.\n",
      "* **Ejercicio 2:** Implemente una tokenizaci\u00f3n de textos en ingl\u00e9s que transforme las contracciones.\n",
      "Cuente el n\u00famero de palabras \u00fanicas con esta implementaci\u00f3n y compare con la estudiada\n",
      "en clase.\n",
      "* **Ejercicio 3:** Re-implemente la tokenizaci\u00f3n estudiada en clases, pero donde el algoritmo sea capaz\n",
      "de entender que *The* y *the* son la misma palabra.\n",
      "* **Ejercicio 4:** Proponga un nuevo conjunto de stopwords, dise\u00f1e una ecuaci\u00f3n para calcularlos\n",
      "autom\u00e1ticamente. (ver la [Ley de Luhn](03.1b-Ley-de-Luhn.ipynb))\n",
      "* **Ejercicio 5:** Encuentre en internet palabras con ra\u00edces lingu\u00edsticas o lemas poco comunes. Pruebe\n",
      "los resultados con el steeming de NLTK. Redise\u00f1e el algoritmo de Poter para que reduzca\n",
      "bien las palabras encontradas por usted.\n",
      "* **Ejercicio 6:** Proponga una lematizaci\u00f3n utilizando diccionarios de palabras existentes en internet.\n",
      "* **Ejercicio 7:** Lea el cap\u00edtulo de m\u00e1quinas de estado de Jurafsky, e implemente una m\u00e1quina parecida\n",
      "en python.\n",
      "* **Ejercicio 8:** Utilice los diccionarios de libreoffice de afijos, infijos, etc e implemente un \n",
      "lematizador que utilice este recurso."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Volver al [*\u00cdndice*](#indice).\n",
      "\n",
      "<a id='referencias'></a>\n",
      "##Referencias##\n",
      "\n",
      "<a id='Bird2009'></a>\n",
      "[1] *[Bird2009]* Steven Bird, Ewan Klain & Edward Loper,. \n",
      "Book **Natural Language Processing with Python**. 2009. \n",
      "p. 10 **ISBN**: 978-0-596-51649-9\n",
      "\n",
      "<a id='Luhn1958'></a>\n",
      "[2] *[Luhn1958]* H.P. Luhn. Paper **The Automatic Creation of Literature Abstract**. \n",
      "*IBM Journal*, 1958.\n",
      "\n",
      "<a id='Gorelick2014'></a>\n",
      "[3] *[Gorelick2014]* Micha Gorelick & Ian Ozsvald. Book **High Performance Python**. \n",
      "O'Reilly. 2014. **ISBN**: 978-1-449-36159-4\n",
      "\n",
      "<a id='Avello2016'></a>\n",
      "[3] *[Avello2016]* Alexander Avello. Thesis **M\u00f3dulo QtNLP_Wordnet para la aplicaci\u00f3n QtNLP**. \n",
      "UCLV. 2016."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a id='alphabetic_index'></a>\n",
      "##\u00cdndice Alfab\u00e9tico##\n",
      "\n",
      "<a id='token'></a>\n",
      "**Token**: se\u00f1al, indicio, muestra. Se usa generalmente para referirse a la unidad\n",
      "m\u00e1s peque\u00f1a de procesamiento: palabras, fonemas, n-grams, etc..\n",
      "\n",
      "\n",
      "Volver al [*\u00cdndice*](#indice)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}